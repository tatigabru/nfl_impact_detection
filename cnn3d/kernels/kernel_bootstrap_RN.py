# AUTOGENERATED SOURCE# DO NOT EDIT
# Imports
from collections import defaultdict, namedtuple
from pytorch_toolbelt.inference.functional import pad_image_tensor
from pytorch_toolbelt.modules import *
import pytorch_toolbelt.modules.encoders as E
import pytorch_toolbelt.modules.decoders as D
from pytorch_toolbelt.inference import tta
from pytorch_toolbelt.utils import image_to_tensor, fs, to_numpy, rgb_image_from_tensor
from pytorch_toolbelt.inference.ensembling import ApplySigmoidTo, Ensembler, ApplySoftmaxTo
from pytorch_toolbelt.inference.functional import pad_image_tensor, unpad_image_tensor
from torch import nn, Tensor
from torch.nn import Parameter
from torch.utils.data import DataLoader, Dataset, ConcatDataset, IterableDataset
from tqdm import tqdm
from functools import partial
from typing import List, Dict, Optional, Tuple, Union, Type, Callable
from torch.utils.data.dataloader import default_collate
import cv2, os, torch, math
import numpy as np
import pandas as pd
import torch.nn.functional as F
import albumentations as A
from torch.utils.data.dataloader import default_collate
from scipy.optimize import linear_sum_assignment
from scipy.ndimage.filters import maximum_filter
import os
import torch
import collections
from torch import nn
import cv2
from scipy.special import softmax
from volumentations import Compose, PadIfNeeded


# Give no chance to randomness
torch.manual_seed(0)
np.random.seed(0)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
from catalyst.registry import Model


INDEX_KEY = "index"
BBOXES_KEY = "bboxes"
LABELS_KEY = "labels"
IMAGE_KEY = "image"
VIDEO_NAME_KEY = "video_id"
VIDEO_FRAME_INDEX_KEY = "frame_index"
HAS_OVERLAP_KEY = "overlap"
CENTERNET_OUTPUT_HEATMAP = "predicted_heatmap"
CENTERNET_OUTPUT_IMPACT_MAP = "predicted_impact"
CENTERNET_OUTPUT_OFFSET = "predicted_offset"
CENTERNET_OUTPUT_SIZE = "predicted_size"
DATASET_MEAN = (0.485, 0.456, 0.406)
DATASET_STD = (0.229, 0.224, 0.225)


CenterNetEncodeResultWithImpact = namedtuple("CenterNetEncodeResultWithImpact", ["heatmap", "classmap", "size", "offset"])
CenterNetDecodeResultWithImpact = namedtuple("CenterNetDecodeResultWithImpact", ["bboxes", "labels", "scores", "objectness"])
VideoInferenceResult = namedtuple("VideoInferenceResult", ["submission", "raw_predictions"])

# Main
KAGGLE_DATA_DIR = '/root/helmet/data/' #os.environ['KAGGLE_2020_NFL']
CLF_CHECKPOINT = 'videoclf_best_f1_0.57.bin'
CHECKPOINT = ['201231_04_03_densenet121_video_fold0_A100_def/checkpoints_metrics_impact_f1/best.pth']
CLIP_LENGTH = 8
ACTIVATION_AFTER = 'after_model' # after_tta, after_ensemble
TTA_MODE = None
USE_SLIDING_WINDOW = False
CLF_THRESH = 0.1
HELMET_THRESHOLD_SCORE = 0.3
IMPACT_THRESHOLD_SCORE = 0.0
TRACKING_IOU_THRESHOLD = 0.1
TRACKING_FRAMES_DISTANCE = 5
COLUMN = 'scores'
USE_FAST_SUBMIT = False
TEST = False
BATCH_SIZE = 1
CLF_BATCH_SIZE = 64
CLF_MODEL_NAME = 'resnet'

# Functions


def bes_radius(det_size, min_overlap=0.5) -> int:
    """Compute radius of gaussian.
    Arguments:
        w (int): weight of box.
        h (int): height of box.
        iou_threshold (float): min required IOU between gt and smoothed box.
    Returns:
        radius (int): radius of gaussian.
    """
    w, h = det_size
    phi = compute_phi(w, h)
    sin_phi = math.sin(math.radians(phi))
    cos_phi = math.cos(math.radians(phi))
    a = sin_phi * cos_phi
    b = -(w * sin_phi + h * cos_phi)
    c = w * h * (1 - min_overlap) / (1 + min_overlap)
    d = math.sqrt(b * b - 4 * a * c)
    r = -(b + d) / (2 * a)
    return int(max(1, math.ceil(r)))


def draw_umich_gaussian(heatmap: np.ndarray, center: Tuple[int, int], radius: int, k=1) -> np.ndarray:
    if radius == "pointwise":
        gaussian = pointwise_gaussian_2d()
        radius = 1
    else:
        diameter = 2 * radius + 1
        gaussian = centernet_gaussian_2d((diameter, diameter), sigma=diameter / 6)

    x, y = int(center[0]), int(center[1])

    height, width = heatmap.shape[0:2]

    left, right = min(x, radius), min(width - x, radius + 1)
    top, bottom = min(y, radius), min(height - y, radius + 1)

    masked_heatmap = heatmap[y - top : y + bottom, x - left : x + right]
    masked_gaussian = gaussian[radius - top : radius + bottom, radius - left : radius + right]
    if min(masked_gaussian.shape) > 0 and min(masked_heatmap.shape) > 0:  # TODO debug
        np.maximum(masked_heatmap, masked_gaussian * k, out=masked_heatmap)
    return heatmap


def centernet_heatmap_nms(heatmap: Tensor, kernel=3) -> Tensor:
    pad = (kernel - 1) // 2

    hmax = torch.nn.functional.max_pool2d(heatmap, (kernel, kernel), stride=1, padding=pad)
    keep = (hmax == heatmap).type_as(heatmap)
    return heatmap * keep


def compute_phi(w, h):
    a, b = min(w, h), max(w, h)
    aspect_ratio = a / b
    angle = 45 * aspect_ratio
    if w > h:
        angle = 90 - angle
    return angle


def pointwise_gaussian_2d():
    pos_kernel = np.float32([[0.5, 0.75, 0.5], [0.75, 1.0, 0.75], [0.5, 0.75, 0.5]])
    return pos_kernel


def centernet_gaussian_2d(shape, sigma=1):
    m, n = [(ss - 1.0) / 2.0 for ss in shape]
    y, x = np.ogrid[-m : m + 1, -n : n + 1]

    h = np.exp(-(x * x + y * y) / (2 * sigma * sigma))
    h[h < np.finfo(h.dtype).eps * h.max()] = 0
    return h


def centernet_topk(scores: Tensor, top_k: int = 100) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:
    batch, cat, height, width = scores.size()

    topk_scores, topk_inds = torch.topk(scores.view(batch, cat, -1), top_k)

    topk_inds = topk_inds % (height * width)
    topk_ys = (topk_inds // width).int().float()
    topk_xs = (topk_inds % width).int().float()

    topk_score, topk_ind = torch.topk(topk_scores.view(batch, -1), top_k)
    topk_clses = (topk_ind // top_k).int()
    topk_inds = centernet_gather_feat(topk_inds.view(batch, -1, 1), topk_ind).view(batch, top_k)
    topk_ys = centernet_gather_feat(topk_ys.view(batch, -1, 1), topk_ind).view(batch, top_k)
    topk_xs = centernet_gather_feat(topk_xs.view(batch, -1, 1), topk_ind).view(batch, top_k)

    return topk_score, topk_inds, topk_clses, topk_ys, topk_xs


def centernet_gather_feat(feat, ind, mask=None):
    dim = feat.size(2)
    ind = ind.unsqueeze(2).expand(ind.size(0), ind.size(1), dim)
    feat = feat.gather(1, ind)
    if mask is not None:
        mask = mask.unsqueeze(2).expand_as(feat)
        feat = feat[mask]
        feat = feat.view(-1, dim)
    return feat


def centernet_tranpose_and_gather_feat(feat, ind):
    feat = feat.permute(0, 2, 3, 1).contiguous()
    feat = feat.view(feat.size(0), -1, feat.size(3))
    feat = centernet_gather_feat(feat, ind)
    return feat


class CenterNetBoxCoderWithImpact:
    """
    CenterNet box coder with separate helmet map & impact map.
    The difference from vanilla box coder is that heatmap nps does not affect class scores probabilities for other classes.
    """

    def __init__(
        self,
        num_classes: int,
        image_size: Tuple[int, int],
        output_stride: int,
        max_objects: int = 128,
        pointwise=False,
        impacts_encoding: str = "circle",
    ):
        image_size = image_size[:2]
        if num_classes != 1:
            raise ValueError()

        if impacts_encoding not in {"point", "heatmap", "circle", "circle_ignore", "box", "box_ignore"}:
            raise KeyError(impacts_encoding)

        self.impacts_encoding = impacts_encoding
        self.num_classes = num_classes
        self.output_stride = output_stride
        self.image_size = image_size
        self.image_height, self.image_width = image_size
        self.max_objects = max_objects
        self.pointwise = pointwise

    def __repr__(self):
        return (
            f"CenterNetBoxCoderWithImpact(num_classes={self.num_classes}, "
            f"image_size={self.image_size}, "
            f"stride={self.output_stride}, "
            f"max_objects={self.max_objects}, "
            f"impacts_encoding={self.impacts_encoding}, "
            f"pointwise={self.pointwise})"
        )

    def box_coder_for_image_size(self, image_size):
        image_size = image_size[:2]
        if self.image_height == image_size[0] and self.image_width == image_size[1]:
            return self
        return CenterNetBoxCoderWithImpact(
            num_classes=self.num_classes,
            image_size=image_size,
            output_stride=self.output_stride,
            max_objects=self.max_objects,
            pointwise=self.pointwise,
            impacts_encoding=self.impacts_encoding,
        )

    def encode(self, bboxes: np.ndarray, labels: np.ndarray) -> CenterNetEncodeResultWithImpact:
        """
        :param bboxes [N,4]
        :param labels [N]
        """

        output_height = self.image_height // self.output_stride
        output_width = self.image_width // self.output_stride

        objectness_map = np.zeros((1, output_height, output_width), dtype=np.float32)
        class_map = np.zeros((1, output_height, output_width), dtype=np.float32)
        size_map = np.zeros((2, output_height, output_width), dtype=np.float32)
        offset_map = np.zeros((2, output_height, output_width), dtype=np.float32)

        if self.impacts_encoding.endswith("_ignore"):
            from ...dataset import IGNORE_INDEX

            class_map.fill(IGNORE_INDEX)

        bboxes = bboxes.astype(np.float32) / float(self.output_stride)

        num_objs = len(bboxes)
        for i in range(num_objs):
            bbox = np.array(bboxes[i], dtype=np.float32)

            h, w = bbox[3] - bbox[1], bbox[2] - bbox[0]
            bbox[[0, 2]] = np.clip(bbox[[0, 2]], 0, output_width - 1)
            bbox[[1, 3]] = np.clip(bbox[[1, 3]], 0, output_height - 1)

            # radius = gaussian_radius((math.ceil(h), math.ceil(w)))
            if self.pointwise:
                radius = "pointwise"
            else:
                radius = bes_radius((h, w))

            _center = np.array([(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2], dtype=np.float32)
            _bbox_int = bbox.astype(int)
            _center_int = _center.astype(int)

            draw_umich_gaussian(objectness_map[0], _center_int, radius)

            if self.impacts_encoding == "point":
                if labels[i]:
                    class_map[0, _center_int[1], _center_int[0]] = 1
            elif self.impacts_encoding == "heatmap":
                if labels[i]:
                    draw_umich_gaussian(class_map[0], _center_int, int(min(w, h) / 2.0 + 0.5))
            elif self.impacts_encoding in {"circle", "circle_ignore"}:
                cv2.circle(
                    class_map[0],
                    tuple(_center_int),
                    color=int(labels[i]),
                    radius=max(1, int(min(w * 0.5, h * 0.5))),
                    thickness=cv2.FILLED,
                )
            elif self.impacts_encoding in {"box", "box_ignore"}:
                cv2.rectangle(
                    class_map[0],
                    (_bbox_int[0], _bbox_int[1]),
                    (_bbox_int[2], _bbox_int[3]),
                    color=int(labels[i]),
                    thickness=cv2.FILLED,
                )
            else:
                raise KeyError(self.impacts_encoding)

            size_map[0, _center_int[1], _center_int[0]] = 1.0 * w
            size_map[1, _center_int[1], _center_int[0]] = 1.0 * h

            offset_map[0, _center_int[1], _center_int[0]] = _center[0] - _center_int[0]
            offset_map[1, _center_int[1], _center_int[0]] = _center[1] - _center_int[1]

        return CenterNetEncodeResultWithImpact(heatmap=objectness_map, classmap=class_map, size=size_map, offset=offset_map)

    @torch.no_grad()
    def decode(
        self,
        heatmap: Tensor,
        classmap: Tensor,
        size_map: Tensor,
        offset_map: Tensor,
        K: Optional[int] = None,
        apply_activation=False,
    ) -> CenterNetDecodeResultWithImpact:
        """
        Decode CenterNet predictions
        :param obj_map: [B, 1, H, W]
        :param cls_map: [B, C, H, W]
        :param size_map: [B, 2, H, W]
        :param offset_map: [B, 2, H, W]
        :param K: Maximum number of objects
        :param apply_sigmoid:
        :return: Tuple of 4 elements (bboxes, labels, obj_scores, cls_scores)
            - [B, K, 4]
            - [B, K]
            - [B, K]
            - [B, K]
        """
        batch, num_classes, height, width = classmap.size()

        if apply_activation:
            heatmap = heatmap.sigmoid()
            classmap = classmap.sigmoid()

        if K is None:
            K = self.max_objects

        obj_map = centernet_heatmap_nms(heatmap)

        # Limit K to prevent having K more W * H
        K = min(K, height * width)

        obj_scores, inds, _, ys, xs = centernet_topk(obj_map, top_k=K)
        if offset_map is not None:
            offset_map = centernet_tranpose_and_gather_feat(offset_map, inds)
            xs = xs.view(batch, K, 1) + offset_map[:, :, 0:1]
            ys = ys.view(batch, K, 1) + offset_map[:, :, 1:2]
        else:
            xs = xs.view(batch, K, 1) + 0.5
            ys = ys.view(batch, K, 1) + 0.5

        size_map = centernet_tranpose_and_gather_feat(size_map, inds)
        classes_map = centernet_tranpose_and_gather_feat(classmap, inds)

        class_scores, class_labels = classes_map.max(dim=2)

        bboxes = (
            torch.cat(
                [
                    xs - size_map[..., 0:1] / 2,
                    ys - size_map[..., 1:2] / 2,
                    xs + size_map[..., 0:1] / 2,
                    ys + size_map[..., 1:2] / 2,
                ],
                dim=2,
            )
            * self.output_stride
        )

        return CenterNetDecodeResultWithImpact(bboxes=bboxes, objectness=obj_scores, labels=class_labels, scores=class_scores)


class CenterNetHead(nn.Module):
    def __init__(
        self,
        input_channels: int,
        embedding_size: int,
        num_classes: int = 1,
        dropout_rate=0.25,
        activation=ACT_RELU,
    ):
        activation_block = get_activation_block(activation)

        super().__init__()
        self.num_classes = num_classes
        self.heatmap_dropout = nn.Dropout2d(dropout_rate)

        self.conv = nn.Conv2d(input_channels, embedding_size, kernel_size=3, padding=1, bias=False)
        self.bn = nn.BatchNorm2d(embedding_size)
        self.act = activation_block(inplace=True)

        self.heatmap = nn.Conv2d(embedding_size, self.num_classes, kernel_size=(3, 3), padding=1)
        self.heatmap.bias.data.fill_(-1.0)

        self.impact = nn.Conv2d(embedding_size, self.num_classes, kernel_size=(3, 3), padding=1)
        self.impact.bias.data.fill_(-1.0)

        self.size = nn.Conv2d(embedding_size, 2, kernel_size=(3, 3), padding=1)
        self.offset = nn.Conv2d(embedding_size, 2, kernel_size=(3, 3), padding=1)

    def forward(self, features: torch.Tensor):
        features = self.heatmap_dropout(features)
        features = self.conv(features)
        features = self.bn(features)
        features = self.act(features)
        output = {
            CENTERNET_OUTPUT_HEATMAP: self.heatmap(features),
            # Size is always positive
            CENTERNET_OUTPUT_SIZE: F.relu(self.size(features), inplace=True),
            # Offset is always in range [0..1)
            CENTERNET_OUTPUT_OFFSET: self.offset(features).clamp(0, 1),
        }
        return output


class CenterNetSimpleHead(nn.Module):
    def __init__(
        self,
        input_channels: int,
        embedding_size: int,
        num_classes: int = 1,
        dropout_rate=0.0,
        activation=ACT_RELU,
    ):
        activation_block = get_activation_block(activation)
        super().__init__()
        if input_channels != embedding_size:
            self.project = nn.Sequential(
                nn.Dropout2d(dropout_rate, inplace=True),
                nn.Conv2d(input_channels, embedding_size, kernel_size=3, padding=1, bias=False),
                nn.BatchNorm2d(embedding_size),
                activation_block(inplace=False),
            )
        else:
            self.project = nn.Dropout2d(dropout_rate, inplace=True)

        self.heatmap = nn.Conv2d(embedding_size, out_channels=num_classes, kernel_size=3, padding=1)
        self.heatmap.bias.data.fill_(0.0)

        self.head_width_height = nn.Conv2d(embedding_size, 2, kernel_size=3, padding=1)
        self.head_offset_regularizer = nn.Conv2d(embedding_size, 2, kernel_size=3, padding=1)

    def forward(self, features: torch.Tensor):
        features = self.project(features)

        output = {
            CENTERNET_OUTPUT_HEATMAP: self.heatmap(features),
            # Size is always positive
            CENTERNET_OUTPUT_SIZE: F.relu(self.head_width_height(features), inplace=True),
            # Offset is always in range [0..1)
            CENTERNET_OUTPUT_OFFSET: self.head_offset_regularizer(features).clamp(0, 1),
        }
        return output


class CenterNetHeadV2(nn.Module):
    def __init__(
        self,
        input_channels: int,
        embedding_size: int,
        num_classes: int,
        dropout_rate=0.25,
        activation=ACT_RELU,
        inplace=True,
    ):
        super().__init__()
        self.num_classes = num_classes
        self.dropout = nn.Dropout2d(dropout_rate, inplace=inplace)

        activation_block = get_activation_block(activation)

        self.heatmap_neck = nn.Sequential(
            nn.Conv2d(input_channels, embedding_size, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(embedding_size),
            activation_block(inplace=True),
        )

        self.heatmap = nn.Conv2d(embedding_size, out_channels=num_classes, kernel_size=1, padding=0)
        self.heatmap.bias.data.fill_(-2.0)

        self.size_head = nn.Sequential(
            nn.Conv2d(input_channels, embedding_size, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(embedding_size),
            activation_block(inplace=True),
            nn.Conv2d(embedding_size, 2, kernel_size=1),
        )

        self.offset_head = nn.Sequential(
            nn.Conv2d(input_channels, embedding_size, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(embedding_size),
            activation_block(inplace=True),
            nn.Conv2d(embedding_size, 2, kernel_size=1),
        )

    def forward(self, features: torch.Tensor):
        features = self.dropout(features)

        features_tail = self.heatmap_neck(features)
        output = {
            CENTERNET_OUTPUT_HEATMAP: self.heatmap(features_tail),
            # Size is always positive
            CENTERNET_OUTPUT_SIZE: F.relu(self.size_head(features), inplace=True),
            # Offset is always in range [0..1)
            CENTERNET_OUTPUT_OFFSET: self.offset_head(features).clamp(0, 1),
        }
        return output


class CenterNetVideoHead(nn.Module):
    def __init__(self, input_channels: int, num_frames: int, activation=ACT_RELU, dropout=0.0):
        super().__init__()
        self.num_frames = num_frames

        activation_block = get_activation_block(activation)

        self.dropout = nn.Dropout2d(dropout, inplace=True)

        self.heatmap = nn.Conv2d(input_channels, out_channels=num_frames, kernel_size=1, padding=0)
        self.heatmap.bias.data.fill_(-2.0)

        self.impact = nn.Conv2d(input_channels, out_channels=num_frames, kernel_size=1, padding=0)
        self.impact.bias.data.fill_(-2.0)

        self.size_head = nn.Sequential(
            nn.Conv2d(input_channels, input_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(input_channels),
            activation_block(inplace=True),
            nn.Conv2d(input_channels, 2 * num_frames, kernel_size=1),
        )

        self.offset_head = nn.Sequential(
            nn.Conv2d(input_channels, input_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(input_channels),
            activation_block(inplace=True),
            nn.Conv2d(input_channels, 2 * num_frames, kernel_size=1),
        )

    def forward(self, features: torch.Tensor):
        bs, _, rows, cols = features.size()

        output = {
            CENTERNET_OUTPUT_HEATMAP: self.heatmap(features).view((bs, self.num_frames, 1, rows, cols)),
            CENTERNET_OUTPUT_IMPACT_MAP: self.impact(features).view((bs, self.num_frames, 1, rows, cols)),
            # Size is always positive
            CENTERNET_OUTPUT_SIZE: F.relu(self.size_head(features), inplace=True).view((bs, self.num_frames, 2, rows, cols)),
            # Offset is always in range [0..1)
            CENTERNET_OUTPUT_OFFSET: self.offset_head(features).clamp(0, 1).view((bs, self.num_frames, 2, rows, cols)),
        }
        return output


class CenterNetHeadV2WithObjectness(nn.Module):
    def __init__(
        self,
        input_channels: int,
        embedding_size: int,
        num_classes: int,
        dropout_rate=0.25,
        activation=ACT_RELU,
        inplace=True,
    ):
        super().__init__()
        self.num_classes = num_classes
        self.dropout = nn.Dropout2d(dropout_rate, inplace=inplace)

        activation_block = get_activation_block(activation)

        self.heatmap_neck = nn.Sequential(
            nn.Conv2d(input_channels, embedding_size, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(embedding_size),
            activation_block(inplace=True),
        )

        self.heatmap = nn.Conv2d(embedding_size, out_channels=1, kernel_size=1, padding=0)
        self.heatmap.bias.data.fill_(-2.0)

        self.classmap = nn.Conv2d(embedding_size, out_channels=num_classes, kernel_size=1, padding=0)
        self.classmap.bias.data.fill_(-2.0)

        self.size_head = nn.Sequential(
            nn.Conv2d(input_channels, embedding_size, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(embedding_size),
            activation_block(inplace=True),
            nn.Conv2d(embedding_size, 2, kernel_size=1),
        )

        self.offset_head = nn.Sequential(
            nn.Conv2d(input_channels, embedding_size, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(embedding_size),
            activation_block(inplace=True),
            nn.Conv2d(embedding_size, 2, kernel_size=1),
        )

    def forward(self, features: torch.Tensor):
        features = self.dropout(features)

        features_tail = self.heatmap_neck(features)
        output = {
            CENTERNET_OUTPUT_HEATMAP: self.heatmap(features_tail),
            CENTERNET_OUTPUT_IMPACT_MAP: self.classmap(features_tail),
            # Size is always positive
            CENTERNET_OUTPUT_SIZE: F.relu(self.size_head(features), inplace=True),
            # Offset is always in range [0..1)
            CENTERNET_OUTPUT_OFFSET: self.offset_head(features).clamp(0, 1),
        }
        return output


class LightweightUnetBlock(nn.Module):
    def __init__(self, in_channels: int, out_channels: int, abn_block=ABN):
        super().__init__()
        self.conv1 = DepthwiseSeparableConv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1, bias=False)
        self.abn1 = abn_block(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1, bias=False)
        self.abn2 = abn_block(out_channels)
        self.conv3 = DepthwiseSeparableConv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1, bias=False)
        self.abn3 = abn_block(out_channels)

    def forward(self, x):
        x = self.conv1(x)
        x = self.abn1(x)
        x = self.conv2(x)
        x = self.abn2(x)
        x = self.conv3(x)
        x = self.abn3(x)
        return x


class ResidualUnetBlock(nn.Module):
    def __init__(self, in_channels: int, out_channels: int, abn_block=ABN):
        super().__init__()
        self.identity = nn.Conv2d(in_channels, out_channels, kernel_size=1)

        self.conv1 = DepthwiseSeparableConv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1, bias=False)
        self.abn1 = abn_block(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1, bias=False)
        self.abn2 = abn_block(out_channels)
        self.conv3 = DepthwiseSeparableConv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1, bias=False)
        self.abn3 = abn_block(out_channels)

    def forward(self, x):
        residual = self.identity(x)

        x = self.conv1(x)
        x = self.abn1(x)
        x = self.conv2(x)
        x = self.abn2(x)
        x = self.conv3(x)
        x = self.abn3(x)
        return x + residual


class DenseNetUnetBlock(nn.Module):
    def __init__(self, in_channels: int, out_channels: int, abn_block=ABN):
        super().__init__()
        self.conv1 = ResidualUnetBlock(in_channels, out_channels, abn_block=abn_block)
        self.conv2 = ResidualUnetBlock(in_channels + out_channels, out_channels, abn_block=abn_block)

    def forward(self, x):
        y = self.conv1(x)
        x = self.conv2(torch.cat([x, y], dim=1))
        return x


class CenterNetUNet(nn.Module):
    def __init__(
        self,
        encoder: E.EncoderModule,
        num_classes: int,
        image_size,
        decoder_channels: List[int],
        embedding_dim=256,
        abn_block=ABN,
        activation=ACT_RELU,
        dropout=0.2,
        box_coder_impacts_encoding="circle",
        head: Union[Type[CenterNetHeadV2], Type[CenterNetHead]] = CenterNetHeadV2WithObjectness,
    ):
        super().__init__()
        self.output_stride = encoder.strides[0]

        abn_block = partial(abn_block, activation=activation)
        unet_block = partial(UnetBlock, abn_block=abn_block)

        self.num_classes = num_classes
        self.encoder = encoder
        self.decoder = D.UNetDecoder(encoder.channels, decoder_channels, unet_block=unet_block, upsample_block=nn.Upsample)
        self.head = head(
            self.decoder.output_filters[0],
            embedding_size=embedding_dim,
            num_classes=num_classes,
            dropout_rate=dropout,
            inplace=False,
            activation=activation,
        )
        self.box_coder = CenterNetBoxCoderWithImpact(
            num_classes=num_classes,
            image_size=image_size,
            output_stride=self.output_stride,
            impacts_encoding=box_coder_impacts_encoding,
        )

    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        feature_maps = self.encoder(x)
        feature_maps = self.decoder(feature_maps)
        output = self.head(feature_maps[0])
        return output


class CenterNetSiameseUNet(nn.Module):
    def __init__(
        self,
        encoder: E.EncoderModule,
        num_classes: int,
        clip_length: int,
        image_size,
        decoder_channels: List[int],
        abn_block=ABN,
        activation=ACT_RELU,
        dropout=0.2,
        head=CenterNetVideoHead,
        box_coder_impacts_encoding="point",
        unet_block: Union[Type[LightweightUnetBlock], Type[UnetBlock], Type[DenseNetUnetBlock]] = UnetBlock,
    ):
        super().__init__()
        self.output_stride = encoder.strides[0]

        abn_block = partial(abn_block, activation=activation)
        unet_block = partial(unet_block, abn_block=abn_block)

        feature_maps = [clip_length * fm for fm in encoder.channels]

        self.num_classes = num_classes
        self.encoder = encoder
        self.decoder = D.UNetDecoder(feature_maps, decoder_channels, unet_block=unet_block, upsample_block=nn.Upsample)
        self.head = head(self.decoder.output_filters[0], num_frames=clip_length, activation=activation, dropout=dropout)
        self.box_coder = CenterNetBoxCoderWithImpact(
            num_classes=num_classes,
            image_size=image_size,
            output_stride=self.output_stride,
            impacts_encoding=box_coder_impacts_encoding,
        )

    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:

        batch_size, num_frames, channels, height, width = x.size()

        feature_maps = self.encoder(x.view(batch_size * num_frames, channels, height, width))

        # this needs testing
        feature_maps = [fm.view(batch_size, num_frames * fm.size(1), fm.size(2), fm.size(3)) for fm in feature_maps]
        feature_maps = self.decoder(feature_maps)
        output = self.head(feature_maps[0])

        return output


@Model
def centernet_b2_unet_s4(
    num_classes, image_size, clip_length=1, dropout=0.1, pretrained=True, box_coder_impacts_encoding="point"
):
    encoder = E.B2Encoder(layers=[1, 2, 3, 4], pretrained=pretrained)
    num_channels = clip_length * 3
    if num_channels != 3:
        encoder.change_input_channels(num_channels)

    return CenterNetUNet(
        encoder,
        num_classes=num_classes,
        decoder_channels=[64, 128, 256],
        image_size=image_size,
        dropout=dropout,
        activation=ACT_SWISH,
        box_coder_impacts_encoding=box_coder_impacts_encoding,
    )


@Model
def centernet_b5_unet_s4(
    num_classes, image_size, dropout=0.2, pretrained=True, clip_length=1, box_coder_impacts_encoding="point"
):
    encoder = E.B5Encoder(layers=[1, 2, 3, 4], pretrained=pretrained)
    num_channels = clip_length * 3
    if num_channels != 3:
        encoder.change_input_channels(num_channels)

    return CenterNetUNet(
        encoder,
        num_classes=num_classes,
        image_size=image_size,
        decoder_channels=[256, 384, 512],
        embedding_dim=256,
        dropout=dropout,
        activation=ACT_SWISH,
        box_coder_impacts_encoding=box_coder_impacts_encoding,
    )


@Model
def videonet_b2_unet_s4(
    num_classes, clip_length=8, image_size=(512, 512), dropout=0.2, pretrained=True, box_coder_impacts_encoding="point"
):
    encoder = E.B2Encoder(layers=[1, 2, 3, 4], pretrained=pretrained)
    return CenterNetSiameseUNet(
        encoder,
        clip_length=clip_length,
        num_classes=num_classes,
        decoder_channels=[16 * clip_length, 32 * clip_length, 64 * clip_length],
        image_size=image_size,
        dropout=dropout,
        box_coder_impacts_encoding=box_coder_impacts_encoding,
        activation=ACT_SWISH,
    )


@Model
def videonet_b5_unet_s4(
    num_classes, clip_length=16, image_size=(512, 512), dropout=0.2, pretrained=True, box_coder_impacts_encoding="point"
):
    encoder = E.B5Encoder(layers=[1, 2, 3, 4], pretrained=pretrained)
    return CenterNetSiameseUNet(
        encoder,
        clip_length=clip_length,
        num_classes=num_classes,
        decoder_channels=[16 * clip_length, 32 * clip_length, 48 * clip_length],
        image_size=image_size,
        dropout=dropout,
        box_coder_impacts_encoding=box_coder_impacts_encoding,
        unet_block=LightweightUnetBlock,
    )


@Model
def videonet_b5_unet_s8(
    num_classes, clip_length=8, image_size=(512, 512), dropout=0.2, pretrained=True, box_coder_impacts_encoding="point"
):
    encoder = E.B5Encoder(layers=[2, 3, 4], pretrained=pretrained)
    return CenterNetSiameseUNet(
        encoder,
        clip_length=clip_length,
        num_classes=num_classes,
        decoder_channels=[16 * clip_length, 32 * clip_length],
        image_size=image_size,
        dropout=dropout,
        box_coder_impacts_encoding=box_coder_impacts_encoding,
        unet_block=LightweightUnetBlock,
        activation=ACT_SWISH,
    )


@Model
def videonet_densenet121_unet_s4(
    num_classes, clip_length=8, image_size=(512, 512), dropout=0.2, pretrained=True, box_coder_impacts_encoding="point"
):
    encoder = E.DenseNet121Encoder(layers=[1, 2, 3, 4], pretrained=pretrained)
    return CenterNetSiameseUNet(
        encoder,
        clip_length=clip_length,
        num_classes=num_classes,
        decoder_channels=[8 * clip_length, 16 * clip_length, 32 * clip_length],
        image_size=image_size,
        dropout=dropout,
        box_coder_impacts_encoding=box_coder_impacts_encoding,
        unet_block=LightweightUnetBlock,
        activation=ACT_SWISH,
    )


@Model
def videonet_densenet161_unet_s4(
    num_classes, clip_length=8, image_size=(512, 512), dropout=0.2, pretrained=True, box_coder_impacts_encoding="point"
):
    encoder = E.DenseNet161Encoder(layers=[1, 2, 3, 4], pretrained=pretrained)
    return CenterNetSiameseUNet(
        encoder,
        clip_length=clip_length,
        num_classes=num_classes,
        decoder_channels=[8 * clip_length, 16 * clip_length, 32 * clip_length],
        image_size=image_size,
        dropout=dropout,
        box_coder_impacts_encoding=box_coder_impacts_encoding,
        unet_block=LightweightUnetBlock,
        activation=ACT_SWISH,
    )


@Model
def videonet_densenet201_unet_s4(
    num_classes, clip_length=8, image_size=(512, 512), dropout=0.2, pretrained=True, box_coder_impacts_encoding="point"
):
    encoder = E.DenseNet201Encoder(layers=[1, 2, 3, 4], pretrained=pretrained)
    return CenterNetSiameseUNet(
        encoder,
        clip_length=clip_length,
        num_classes=num_classes,
        decoder_channels=[8 * clip_length, 16 * clip_length, 32 * clip_length],
        image_size=image_size,
        dropout=dropout,
        box_coder_impacts_encoding=box_coder_impacts_encoding,
        unet_block=LightweightUnetBlock,
        activation=ACT_SWISH,
    )


@Model
def videonet_densenet169_unet_s4(
    num_classes, clip_length=8, image_size=(512, 512), dropout=0.2, pretrained=True, box_coder_impacts_encoding="point"
):
    encoder = E.DenseNet169Encoder(layers=[1, 2, 3, 4], pretrained=pretrained)
    return CenterNetSiameseUNet(
        encoder,
        clip_length=clip_length,
        num_classes=num_classes,
        decoder_channels=[8 * clip_length, 16 * clip_length, 32 * clip_length],
        image_size=image_size,
        dropout=dropout,
        box_coder_impacts_encoding=box_coder_impacts_encoding,
        unet_block=LightweightUnetBlock,
        activation=ACT_SWISH,
    )


@Model
def videonet_srx50_unet_s4(
    num_classes, clip_length=8, image_size=(512, 512), dropout=0.2, pretrained=True, box_coder_impacts_encoding="point"
):
    encoder = E.SEResNeXt50Encoder(layers=[1, 2, 3, 4], pretrained=pretrained)
    return CenterNetSiameseUNet(
        encoder,
        clip_length=clip_length,
        num_classes=num_classes,
        decoder_channels=[8 * clip_length, 16 * clip_length, 32 * clip_length],
        image_size=image_size,
        dropout=dropout,
        box_coder_impacts_encoding=box_coder_impacts_encoding,
        unet_block=LightweightUnetBlock,
        activation=ACT_RELU,
    )


@Model
def videonet_dpn92_unet_s4(
    num_classes, clip_length=8, image_size=(512, 512), dropout=0.2, pretrained=True, box_coder_impacts_encoding="point"
):
    encoder = E.DPN92Encoder(layers=[0, 1, 2, 3], pretrained=pretrained)
    return CenterNetSiameseUNet(
        encoder,
        clip_length=clip_length,
        num_classes=num_classes,
        decoder_channels=[8 * clip_length, 16 * clip_length, 32 * clip_length],
        image_size=image_size,
        dropout=dropout,
        box_coder_impacts_encoding=box_coder_impacts_encoding,
        unet_block=LightweightUnetBlock,
        activation=ACT_RELU,
    )


@Model
def videonet_dpn92_unet_s8(
    num_classes, clip_length=8, image_size=(512, 512), dropout=0.2, pretrained=True, box_coder_impacts_encoding="point"
):
    encoder = E.DPN92Encoder(layers=[1, 2, 3], pretrained=pretrained)
    return CenterNetSiameseUNet(
        encoder,
        clip_length=clip_length,
        num_classes=num_classes,
        decoder_channels=[16 * clip_length, 32 * clip_length],
        image_size=image_size,
        dropout=dropout,
        box_coder_impacts_encoding=box_coder_impacts_encoding,
        unet_block=LightweightUnetBlock,
        activation=ACT_RELU,
    )


@Model
def videonet_b2_denseunet_s4(
    num_classes, clip_length=8, image_size=(512, 512), dropout=0.2, pretrained=True, box_coder_impacts_encoding="point"
):
    encoder = E.B2Encoder(layers=[1, 2, 3, 4], pretrained=pretrained)
    return CenterNetSiameseUNet(
        encoder,
        clip_length=clip_length,
        num_classes=num_classes,
        decoder_channels=[8 * clip_length, 16 * clip_length, 32 * clip_length],
        image_size=image_size,
        dropout=dropout,
        box_coder_impacts_encoding=box_coder_impacts_encoding,
        activation=ACT_SWISH,
        unet_block=DenseNetUnetBlock,
    )


class CenterNetVideoHead3D(nn.Module):
    def __init__(self, input_channels: int, num_frames: int, head_features: int, activation=ACT_RELU, dropout=0.0):
        super().__init__()
        self.num_frames = num_frames
        activation_block = get_activation_block(activation)

        self.bottleneck = nn.Sequential(
            OrderedDict(
                [
                    ("conv1", nn.Conv3d(input_channels, head_features, kernel_size=1, bias=False)),
                    ("drop1", nn.Dropout3d(dropout, inplace=True)),
                    ("bn1", nn.BatchNorm3d(head_features)),
                    ("act1", activation_block(inplace=True)),
                    ("conv2", nn.Conv3d(head_features, head_features, kernel_size=3, padding=1, bias=False)),
                    ("bn2", nn.BatchNorm3d(head_features)),
                    ("act2", activation_block(inplace=True)),
                    ("conv3", nn.Conv3d(head_features, head_features, kernel_size=3, padding=1, bias=False)),
                    ("bn3", nn.BatchNorm3d(head_features)),
                    ("act3", activation_block(inplace=True)),
                ]
            )
        )

        self.heatmap = nn.Conv3d(head_features, out_channels=1, kernel_size=1, padding=0)
        self.heatmap.bias.data.fill_(-2.0)

        self.impact = nn.Conv3d(head_features, out_channels=1, kernel_size=1, padding=0)
        self.impact.bias.data.fill_(-1.69)

        self.size_head = nn.Sequential(
            nn.Conv3d(head_features, 64, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm3d(64),
            activation_block(inplace=True),
            nn.Conv3d(64, 2, kernel_size=1),
        )

        self.offset_head = nn.Sequential(
            nn.Conv3d(head_features, 32, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm3d(32),
            activation_block(inplace=True),
            nn.Conv3d(32, 2, kernel_size=1),
        )

    def forward(self, features: torch.Tensor):
        features = self.bottleneck(features)
        bs, _, depth, rows, cols = features.size()

        output = {
            CENTERNET_OUTPUT_HEATMAP: self.heatmap(features).view((bs, self.num_frames, 1, rows, cols)),
            CENTERNET_OUTPUT_IMPACT_MAP: self.impact(features).view((bs, self.num_frames, 1, rows, cols)),
            # Size is always positive
            CENTERNET_OUTPUT_SIZE: F.relu(self.size_head(features), inplace=True).view((bs, self.num_frames, 2, rows, cols)),
            # Offset is always in range [0..1)
            CENTERNET_OUTPUT_OFFSET: self.offset_head(features).clamp(0, 1).view((bs, self.num_frames, 2, rows, cols)),
        }
        return output


class VideoCenterNet3D(nn.Module):
    def __init__(
        self,
        encoder: E.EncoderModule,
        num_classes: int,
        clip_length: int,
        image_size,
        decoder_channels: List[int],
        activation=ACT_RELU,
        dropout=0.1,
        unet_block=LightweightUnetBlock,
        head_features=256,
        head=CenterNetVideoHead3D,
        box_coder_impacts_encoding="circle",
    ):
        super().__init__()
        output_stride = encoder.strides[0]

        self.num_classes = num_classes
        self.encoder = encoder
        self.decoder = D.UNetDecoder(encoder.channels, decoder_channels, unet_block=unet_block, upsample_block=nn.Upsample)
        self.head = head(
            decoder_channels[0], head_features=head_features, num_frames=clip_length, dropout=dropout, activation=activation
        )
        self.box_coder = CenterNetBoxCoderWithImpact(
            num_classes=num_classes,
            image_size=image_size,
            output_stride=output_stride,
            impacts_encoding=box_coder_impacts_encoding,
        )

    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:

        batch_size, num_frames, channels, height, width = x.size()

        feature_maps = self.encoder(x.view(batch_size * num_frames, channels, height, width))
        feature_maps = self.decoder(feature_maps)

        fm = feature_maps[0]
        fm3d = fm.view(batch_size, num_frames, -1, fm.size(2), fm.size(3)).permute(0, 2, 1, 3, 4)

        output = self.head(fm3d)

        return output


@Model
def videonet_b0_3d_s8(num_classes, clip_length, image_size, dropout=0.2, pretrained=True, box_coder_impacts_encoding="circle"):
    encoder = E.B0Encoder(layers=[2, 3, 4], pretrained=pretrained)
    return VideoCenterNet3D(
        encoder,
        clip_length=clip_length,
        num_classes=num_classes,
        image_size=image_size,
        decoder_channels=[64, 128],
        head_features=64,
        dropout=dropout,
        activation=ACT_SWISH,
        box_coder_impacts_encoding=box_coder_impacts_encoding,
    )


@Model
def videonet_densenet121_3d_s8(
    num_classes, clip_length, image_size, dropout=0.2, pretrained=True, box_coder_impacts_encoding="circle"
):
    encoder = E.DenseNet121Encoder(layers=[2, 3, 4], pretrained=pretrained)
    return VideoCenterNet3D(
        encoder,
        clip_length=clip_length,
        num_classes=num_classes,
        decoder_channels=[256, 384],
        image_size=image_size,
        head_features=192,
        dropout=dropout,
        activation=ACT_RELU,
        box_coder_impacts_encoding=box_coder_impacts_encoding,
    )


def get_model(model_name: str, num_classes=1, image_size: Tuple[int, int] = (1024, 1024), pretrained=True, **kwargs):
    from catalyst.dl import registry

    model_fn = registry.MODEL.get(model_name)
    return model_fn(num_classes=num_classes, image_size=image_size, pretrained=pretrained, **kwargs)


def without(d: Dict, key: str) -> Dict:
    new_d = d.copy()
    new_d.pop(key)
    return new_d


def centernet_collate(batch):
    skip_keys = [BBOXES_KEY, LABELS_KEY]
    excluded_items = [dict((k, v) for k, v in b.items() if k in skip_keys) for b in batch]
    included_items = [dict((k, v) for k, v in b.items() if k not in skip_keys) for b in batch]

    batch: dict = default_collate(included_items)
    for k in skip_keys:
        out = [item[k] for item in excluded_items if k in item]
        if len(out):
            batch[k] = out

    return batch


def centernet_video_collate(batch):
    skip_keys = [BBOXES_KEY, LABELS_KEY, VIDEO_NAME_KEY, VIDEO_FRAME_INDEX_KEY]
    excluded_items = [dict((k, v) for k, v in b.items() if k in skip_keys) for b in batch]
    included_items = [dict((k, v) for k, v in b.items() if k not in skip_keys) for b in batch]

    batch: dict = default_collate(included_items)
    for k in skip_keys:
        out = [item[k] for item in excluded_items if k in item]
        if len(out):
            batch[k] = out

    return batch


def real_model(model: nn.Module):
    if isinstance(model, (ApplySigmoidTo, ApplySoftmaxTo)):
        return real_model(model.model)
    if isinstance(model, (tta.GeneralizedTTA, tta.MultiscaleTTA, CenternetClipsMultiscaleTTA, CenternetClipsFlipLRTTA)):
        return real_model(model.model)
    if isinstance(model, (nn.DataParallel, torch.nn.parallel.DistributedDataParallel)):
        return real_model(model.module)
    if isinstance(model, Ensembler):
        return get_box_coder_from_model(model.models[0])
    return model


def get_box_coder_from_model(model):
    return real_model(model).box_coder


def centernet_ms_size_deaugment(
    images: List[Tensor],
    size_offsets: List[Union[int, Tuple[int, int]]],
    reduction: Optional[Union[str, Callable]] = "mean",
    mode: str = "bilinear",
    align_corners: bool = True,
    stride: int = 1,
) -> Tensor:
    if len(images) != len(size_offsets):
        raise ValueError("Number of images must be equal to number of size offsets")

    deaugmented_outputs = []
    for image, offset in zip(images, size_offsets):
        batch_size, channels, rows, cols = image.size()
        # TODO: Add support of tuple (row_offset, col_offset)
        original_size = rows - offset // stride, cols - offset // stride
        scaled_image = torch.nn.functional.interpolate(image, size=original_size, mode=mode, align_corners=align_corners)
        size_scale = torch.tensor(
            [original_size[0] / rows, original_size[1] / cols], dtype=scaled_image.dtype, device=scaled_image.device
        ).view((1, 2, 1, 1))

        deaugmented_outputs.append(scaled_image * size_scale)

    deaugmented_outputs = torch.stack(deaugmented_outputs)
    if reduction == "mean":
        deaugmented_outputs = deaugmented_outputs.mean(dim=0)
    if reduction == "sum":
        deaugmented_outputs = deaugmented_outputs.sum(dim=0)
    if callable(reduction):
        deaugmented_outputs = reduction(deaugmented_outputs, dim=0)

    return deaugmented_outputs


def centernet_fliplr_tta(model: nn.Module, average_heatmap=True):
    return tta.GeneralizedTTA(
        model,
        augment_fn=tta.fliplr_image_augment,
        deaugment_fn={
            CENTERNET_OUTPUT_HEATMAP: partial(tta.fliplr_image_deaugment, reduction="mean" if average_heatmap else "sum"),
            CENTERNET_OUTPUT_IMPACT_MAP: partial(tta.fliplr_image_deaugment, reduction="mean" if average_heatmap else "sum"),
            CENTERNET_OUTPUT_SIZE: tta.fliplr_image_deaugment,
            CENTERNET_OUTPUT_OFFSET: tta.fliplr_image_deaugment,
        },
    )


def centernet_flips_tta(model: nn.Module, average_heatmap=True):
    return tta.GeneralizedTTA(
        model,
        augment_fn=tta.flips_image_augment,
        deaugment_fn={
            CENTERNET_OUTPUT_HEATMAP: partial(tta.flips_image_deaugment, reduction="mean" if average_heatmap else "sum"),
            CENTERNET_OUTPUT_IMPACT_MAP: partial(tta.flips_image_deaugment, reduction="mean" if average_heatmap else "sum"),
            CENTERNET_OUTPUT_SIZE: tta.flips_image_deaugment,
            CENTERNET_OUTPUT_OFFSET: tta.flips_image_deaugment,
        },
    )


def centernet_d2_tta(model: nn.Module, average_heatmap=True):
    return tta.GeneralizedTTA(
        model,
        augment_fn=tta.d2_image_augment,
        deaugment_fn={
            CENTERNET_OUTPUT_HEATMAP: partial(tta.d2_image_deaugment, reduction="mean" if average_heatmap else "sum"),
            CENTERNET_OUTPUT_IMPACT_MAP: partial(tta.d2_image_deaugment, reduction="mean" if average_heatmap else "sum"),
            CENTERNET_OUTPUT_SIZE: tta.d2_image_deaugment,
            CENTERNET_OUTPUT_OFFSET: tta.d2_image_deaugment,
        },
    )


def video_centernet_ms_tta(model: nn.Module, input: Tensor, size_offsets: List[int], average=True) -> Dict[str, Tensor]:
    """

    :param model:
    :param input: Input batch of shape [B,S,C,H,W]
    :return:
    """
    o1 = model(input)
    batch_size, clip_size, channels, rows, cols = input.size()
    original_size = rows, cols

    # output_stride = real_model(model).output_stride
    input_flatten = input.view(batch_size * clip_size, channels, rows, cols)
    ms_inputs = tta.ms_image_augment(input_flatten, size_offsets)
    one_over_n = 1.0 / (1 + len(size_offsets))

    for x in ms_inputs:
        scaled_size = x.size(2), x.size(3)
        x = x.view(batch_size, clip_size, x.size(1), x.size(2), x.size(3))
        o_ms = model(x)

        for fm in [CENTERNET_OUTPUT_HEATMAP, CENTERNET_OUTPUT_IMPACT_MAP, CENTERNET_OUTPUT_SIZE, CENTERNET_OUTPUT_OFFSET]:
            y = o_ms[fm]
            _, _, channels, rows, cols = y.size()

            y = y.view(-1, channels, rows, cols)
            y = torch.nn.functional.interpolate(y, size=(o1[fm].size(3), o1[fm].size(4)), mode="bilinear", align_corners=True)

            if fm in {CENTERNET_OUTPUT_SIZE}:
                size_scale = torch.tensor(
                    [original_size[1] / scaled_size[1], original_size[0] / scaled_size[0]], dtype=y.dtype, device=y.device
                ).view((1, 2, 1, 1))
                y *= size_scale
            y = y.view(batch_size, clip_size, channels, y.size(2), y.size(3))
            o1[fm] += y

    if average:
        o1[CENTERNET_OUTPUT_HEATMAP] *= one_over_n
        o1[CENTERNET_OUTPUT_IMPACT_MAP] *= one_over_n

    o1[CENTERNET_OUTPUT_SIZE] *= one_over_n
    o1[CENTERNET_OUTPUT_OFFSET] *= one_over_n
    return o1


def video_centernet_fliplr_tta(model: nn.Module, input: Tensor, average=True) -> Dict[str, Tensor]:
    """

    :param model:
    :param input: Input batch of shape [B,S,C,H,W]
    :return:
    """
    o1 = model(input)
    o2 = model(clip_fliplr(input))

    for fm in [CENTERNET_OUTPUT_HEATMAP, CENTERNET_OUTPUT_IMPACT_MAP, CENTERNET_OUTPUT_SIZE, CENTERNET_OUTPUT_OFFSET]:
        o1[fm] += clip_fliplr(o2[fm])

    if average:
        o1[CENTERNET_OUTPUT_HEATMAP] *= 0.5
        o1[CENTERNET_OUTPUT_IMPACT_MAP] *= 0.5

    o1[CENTERNET_OUTPUT_SIZE] *= 0.5
    o1[CENTERNET_OUTPUT_OFFSET] *= 0.5
    return o1


def aspect_aware_video_centernet_ms_tta(
    model: nn.Module, input: Tensor, size_offsets: List[int], average=True
) -> Dict[str, Tensor]:
    """
    Multi-scale TTA with aspect-ratio preserving scaling
    :param model:
    :param input: Input batch of shape [B,S,C,H,W]
    :return:
    """
    output_stride = real_model(model).output_stride

    o1 = model(input)
    batch_size, clip_size, channels, rows, cols = input.size()
    original_size = rows, cols

    # output_stride = real_model(model).output_stride
    input_flatten = input.view(batch_size * clip_size, channels, rows, cols)
    ms_inputs = aspect_aware_ms_image_augment(input_flatten, size_offsets)
    one_over_n = 1.0 / (1 + len(size_offsets))

    for x in ms_inputs:
        scaled_size = x.size(2), x.size(3)

        x, ms_pad = pad_image_tensor(x)
        ms_pad_strided = (
            ms_pad[0] // output_stride,
            ms_pad[1] // output_stride,
            ms_pad[2] // output_stride,
            ms_pad[3] // output_stride,
        )

        x = x.view(batch_size, clip_size, x.size(1), x.size(2), x.size(3))
        o_ms = model(x)

        for fm in [CENTERNET_OUTPUT_HEATMAP, CENTERNET_OUTPUT_IMPACT_MAP, CENTERNET_OUTPUT_SIZE, CENTERNET_OUTPUT_OFFSET]:
            y = o_ms[fm]
            _, _, channels, rows, cols = y.size()

            y = y.view(-1, channels, rows, cols)
            y = unpad_image_tensor(y, ms_pad_strided)
            y = torch.nn.functional.interpolate(y, size=(o1[fm].size(3), o1[fm].size(4)), mode="bilinear", align_corners=True)

            if fm in {CENTERNET_OUTPUT_SIZE}:
                size_scale = torch.tensor(
                    [original_size[1] / scaled_size[1], original_size[0] / scaled_size[0]], dtype=y.dtype, device=y.device
                ).view((1, 2, 1, 1))
                y *= size_scale
            y = y.view(batch_size, clip_size, channels, y.size(2), y.size(3))
            o1[fm] += y

    if average:
        o1[CENTERNET_OUTPUT_HEATMAP] *= one_over_n
        o1[CENTERNET_OUTPUT_IMPACT_MAP] *= one_over_n

    o1[CENTERNET_OUTPUT_SIZE] *= one_over_n
    o1[CENTERNET_OUTPUT_OFFSET] *= one_over_n
    return o1


def aspect_aware_ms_image_augment(
    image: Tensor, size_offsets: List[Union[int, Tuple[int, int]]], mode="bilinear", align_corners=True
) -> List[Tensor]:
    """
    Multi-scale image augmentation. This function create list of resized tensors from the input one.
    """
    batch_size, channels, rows, cols = image.size()
    augmented_inputs = []
    aspect = float(rows) / float(cols)

    for offset in size_offsets:
        # TODO: Add support of tuple (row_offset, col_offset)
        if offset == 0:
            augmented_inputs.append(image)
        else:
            scale_size = rows + int(offset * aspect + 0.5), cols + offset
            scaled_input = torch.nn.functional.interpolate(image, size=scale_size, mode=mode, align_corners=align_corners)
            augmented_inputs.append(scaled_input)
    return augmented_inputs


class CenternetClipsMultiscaleTTA(nn.Module):
    def __init__(self, model, size_offsets: List[int], average_heatmap=True):
        super().__init__()
        self.model = model
        self.size_offsets = [offset for offset in size_offsets if offset != 0]
        self.average_heatmap = average_heatmap

    def forward(self, x):
        return video_centernet_ms_tta(self.model, x, self.size_offsets, self.average_heatmap)


class CenternetClipsFlipLRTTA(nn.Module):
    def __init__(self, model, average_heatmap=True):
        super().__init__()
        self.model = model
        self.average_heatmap = average_heatmap

    def forward(self, x):
        return video_centernet_fliplr_tta(self.model, x, average=self.average_heatmap)


class CenternetClipsAspectMultiscaleTTA(nn.Module):
    def __init__(self, model, size_offsets: List[int], average_heatmap=True):
        super().__init__()
        self.model = model
        self.size_offsets = [offset for offset in size_offsets if offset != 0]
        self.average_heatmap = average_heatmap

    def forward(self, x):
        return aspect_aware_video_centernet_ms_tta(self.model, x, self.size_offsets, self.average_heatmap)


def centernet_ms_tta(model: nn.Module, size_offsets: List[int], average_heatmap=True):
    output_stride = real_model(model).output_stride

    return tta.MultiscaleTTA(
        model,
        size_offsets,
        deaugment_fn={
            CENTERNET_OUTPUT_HEATMAP: partial(
                tta.ms_image_deaugment, reduction="mean" if average_heatmap else "sum", stride=output_stride
            ),
            CENTERNET_OUTPUT_IMPACT_MAP: partial(
                tta.ms_image_deaugment, reduction="mean" if average_heatmap else "sum", stride=output_stride
            ),
            CENTERNET_OUTPUT_SIZE: partial(centernet_ms_size_deaugment, stride=output_stride),
            CENTERNET_OUTPUT_OFFSET: partial(tta.ms_image_deaugment, stride=output_stride),
        },
    )


def centernet_d4_tta(model: nn.Module, average_heatmap=True):
    return tta.GeneralizedTTA(
        model,
        augment_fn=tta.d4_image_augment,
        deaugment_fn={
            CENTERNET_OUTPUT_HEATMAP: partial(tta.d4_image_deaugment, reduction="mean" if average_heatmap else "sum"),
            CENTERNET_OUTPUT_IMPACT_MAP: partial(tta.d4_image_deaugment, reduction="mean" if average_heatmap else "sum"),
            CENTERNET_OUTPUT_SIZE: tta.d4_image_deaugment,
            CENTERNET_OUTPUT_OFFSET: tta.d4_image_deaugment,
        },
    )


def wrap_model_with_tta(model: nn.Module, tta_mode: Optional[str]) -> nn.Module:
    if tta_mode is None:
        return model
    elif tta_mode == "fliplr":
        model = centernet_fliplr_tta(model)
    elif tta_mode == "clips-fliplr":
        model = CenternetClipsFlipLRTTA(model)
    elif tta_mode == "clips-ms":
        model = CenternetClipsMultiscaleTTA(model, [0, +128, +256])
    elif tta_mode == "clips-fliplr-ms":
        model = CenternetClipsMultiscaleTTA(CenternetClipsFlipLRTTA(model), [0, +128, +256])
    elif tta_mode == "clips-fliplr-ar-ms":
        model = CenternetClipsAspectMultiscaleTTA(CenternetClipsFlipLRTTA(model), [0, +128, +256])
    elif tta_mode == "clips-fliplr-ar-ms-sum":
        model = CenternetClipsAspectMultiscaleTTA(
            CenternetClipsFlipLRTTA(model, average_heatmap=False), [0, +128, +256], average_heatmap=False
        )
    elif tta_mode == "fliplr-ms":
        model = centernet_fliplr_tta(centernet_ms_tta(model, [0, +128, +256]))
    elif tta_mode == "fliplr-ms-sum":
        model = centernet_fliplr_tta(centernet_ms_tta(model, [0, +128, +256], average_heatmap=False), average_heatmap=False)
    elif tta_mode == "flips":
        model = centernet_flips_tta(model)
    elif tta_mode == "d2":
        model = centernet_d2_tta(model)
    elif tta_mode == "d2-ms":
        model = centernet_d2_tta(centernet_ms_tta(model, [0, -128, +128]))
    elif tta_mode == "ms":
        model = centernet_ms_tta(model, [0, -128, +128])
    else:
        raise KeyError("Unsupported TTA mode")

    return model


class OpenCVFramesDataset(IterableDataset):
    def __init__(self, video: str, normalize=A.Normalize(DATASET_MEAN, DATASET_STD)):
        """

        :param video:
        """
        self.video_fname = video
        self.cap = cv2.VideoCapture(self.video_fname)
        self.num_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))
        self.normalize = normalize

    def __iter__(self):
        retval, image = self.cap.read()
        index = 0

        while retval:
            image = self.normalize(image=image)["image"]
            input = {
                INDEX_KEY: index,
                IMAGE_KEY: image_to_tensor(image),
                VIDEO_NAME_KEY: os.path.basename(self.video_fname),
                VIDEO_FRAME_INDEX_KEY: index + 1,
            }
            yield input

            retval, image = self.cap.read()
            index += 1

    def __len__(self):
        return self.num_frames


def find_videos_in_dir(dirname: str):
    return [fname for fname in fs.find_in_dir(dirname) if has_video_ext(fname)]


def has_video_ext(fname: str) -> bool:
    name, ext = os.path.splitext(fname)
    return ext.lower() in {".mp4"}


def centernet_model_from_checkpoint(
    checkpoint_name: str, strict=True, **extra_model_kwargs
) -> Tuple[torch.nn.Module, Dict, Dict]:
    checkpoint = torch.load(checkpoint_name, map_location="cpu")
    model_state_dict = checkpoint["model_state_dict"]

    cfg = checkpoint["checkpoint_data"]["config"]

    model_name = cfg["model"]["name"]
    config_model_kwargs = without(cfg["model"], "name")
    if extra_model_kwargs is not None:
        config_model_kwargs.update(**extra_model_kwargs)

    model = get_model(model_name, pretrained=False, **config_model_kwargs)
    model.load_state_dict(model_state_dict, strict=strict)
    return model.cuda().eval(), cfg, checkpoint


def ensemble_from_centernet_checkpoints(
    checkpoint_fnames,
    strict=True,
    activation: str = "after_model",
    tta=None,
    sigmoid_outputs=(CENTERNET_OUTPUT_HEATMAP, CENTERNET_OUTPUT_IMPACT_MAP),
    extra_model_kwargs: Dict = None,
):
    if activation not in {"after_model", "after_tta", "after_ensemble"}:
        raise KeyError(activation)

    models = []
    configs = []
    checkpoints = []
    if extra_model_kwargs is None:
        extra_model_kwargs = {}

    for ck in checkpoint_fnames:
        model, config, checkpoint = centernet_model_from_checkpoint(ck, strict=strict, **extra_model_kwargs)
        models.append(model)
        configs.append(config)
        checkpoints.append(checkpoint)

    box_coder = models[0].box_coder

    if activation == "after_model":
        models = [ApplySigmoidTo(m, output_key=sigmoid_outputs) for m in models]

    if len(models) > 1:
        model = Ensembler(models)
        if activation == "after_ensemble":
            model = ApplySigmoidTo(model, output_key=sigmoid_outputs)
    else:
        assert len(models) == 1
        model = models[0]

    if tta is not None:
        model = wrap_model_with_tta(model, tta)
        print("Wrapping models with TTA", tta)

    if activation == "after_tta":
        model = ApplySigmoidTo(model, output_key=sigmoid_outputs)

    return model.eval(), configs, checkpoints, box_coder


def pad_clips_tensor(image_tensor: Tensor, pad_size: Union[int, Tuple[int, int]] = 32):
    clips = []
    for clip in image_tensor:
        clip_with_pad, pad = pad_image_tensor(clip, pad_size)
        clips.append(clip_with_pad)
    return torch.stack(clips), pad


@torch.no_grad()
def run_inference_centernet(
    model,
    box_coder: CenterNetBoxCoderWithImpact,
    video_fname: str,
    batch_size=1,
    helmet_threshold_score=0.5,
    impact_threshold_score=0.5,
    image_size_pad_factor=32,
    debug_video_fname=None,
    apply_activation_when_decode=True,
    fp16=False,
) -> VideoInferenceResult:
    """

    :param model:
    :param box_coder:
    :param video_fname:
    :param batch_size:
    :param helmet_threshold_score:
    :param impact_threshold_score:
    :param image_size_pad_factor:
    :param debug_video_fname:
    :param apply_activation_when_decode: Must be False if model wrapped with ApplySigmoidTo
    :return:
    """
    df = defaultdict(list)

    ds = OpenCVFramesDataset(video_fname)

    debug_video = None
    if debug_video_fname is not None:
        os.makedirs(os.path.dirname(debug_video_fname), exist_ok=True)
        fourcc = cv2.VideoWriter_fourcc(*"mp4v")
        debug_video = cv2.VideoWriter(debug_video_fname, fourcc, 60, (1280, 720))

    raw_predictions = []

    loader = DataLoader(ds, batch_size=batch_size, num_workers=0, pin_memory=True, shuffle=False, drop_last=False)
    for batch in tqdm(loader, desc=os.path.basename(video_fname)):
        image = batch[IMAGE_KEY].cuda(non_blocking=True)
        bs, channels, rows, cols = image.size()

        image_padded, pad = pad_image_tensor(image, image_size_pad_factor)

        with torch.cuda.amp.autocast(fp16):
            output = model(image_padded)

        decoded = box_coder.decode(
            output[CENTERNET_OUTPUT_HEATMAP],
            output[CENTERNET_OUTPUT_IMPACT_MAP],
            output[CENTERNET_OUTPUT_SIZE],
            output[CENTERNET_OUTPUT_OFFSET],
            apply_activation=apply_activation_when_decode,
        )

        for i in range(bs):
            video = str(batch[VIDEO_NAME_KEY][i])
            frame_index = int(batch[VIDEO_FRAME_INDEX_KEY][i])

            gameKey, playID, view = fs.id_from_fname(video).split("_")

            bboxes = decoded.bboxes
            # Unpad
            bboxes = bboxes - torch.tensor([pad[0], pad[2], pad[0], pad[2]], device=bboxes.device, dtype=bboxes.dtype).reshape(
                (1, 1, 4)
            )
            # Clamp
            bboxes.clamp_min_(0)
            bboxes[:, :, 0].clamp_max_(cols)  # X
            bboxes[:, :, 1].clamp_max_(rows)  # Y
            bboxes[:, :, 2].clamp_max_(cols)  # X
            bboxes[:, :, 3].clamp_max_(rows)  # Y

            raw_predictions.append(
                {
                    "frame": frame_index,
                    "gameKey": gameKey,
                    "playID": playID,
                    "video": video,
                    "view": view,
                    "bboxes": to_numpy(bboxes[i]),
                    "helmet_probas": to_numpy(decoded.objectness[i]),
                    "impact_probas": to_numpy(decoded.scores[i]),
                }
            )

            # Select & filter
            confident_helmet_mask = decoded.objectness[i] >= helmet_threshold_score
            confident_impact_mask = confident_helmet_mask & (decoded.scores[i] >= impact_threshold_score)

            helmet_bboxes = to_numpy(bboxes[i, confident_helmet_mask]).astype(int)
            helmet_probas = to_numpy(decoded.objectness[i, confident_helmet_mask])

            impact_bboxes = to_numpy(bboxes[i, confident_impact_mask]).astype(int)
            impact_probas = to_numpy(decoded.scores[i, confident_impact_mask])

            if len(impact_bboxes):
                for (x1, y1, x2, y2), impact_p in zip(impact_bboxes, impact_probas):
                    df["gameKey"].append(gameKey)
                    df["playID"].append(int(playID))
                    df["view"].append(view)
                    df["video"].append(video)
                    df["frame"].append(frame_index)  # Seems that frames starts from 1
                    df["left"].append(x1)
                    df["width"].append(x2 - x1)
                    df["top"].append(y1)
                    df["scores"].append(impact_p)
                    df["height"].append(y2 - y1)

            if debug_video_fname is not None:
                image = rgb_image_from_tensor(batch[IMAGE_KEY][i], DATASET_MEAN, DATASET_STD)
                image = np.ascontiguousarray(image)

                for (x1, y1, x2, y2), helmet_p in zip(helmet_bboxes, helmet_probas):
                    cv2.rectangle(image, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)
                    cv2.putText(
                        image,
                        text=f"{helmet_p:.2f}",
                        org=(x1, y1 - 5),
                        fontFace=cv2.FONT_HERSHEY_PLAIN,
                        fontScale=1,
                        color=(0, 255, 0),
                        thickness=1,
                        lineType=cv2.LINE_AA,
                    )

                for (x1, y1, x2, y2), impact_p in zip(impact_bboxes, impact_probas):
                    cv2.rectangle(image, (x1, y1), (x2, y2), color=(0, 0, 255), thickness=2)
                    cv2.putText(
                        image,
                        text=f"{impact_p:.2f}",
                        org=(x1, y2 + 10),
                        fontFace=cv2.FONT_HERSHEY_PLAIN,
                        fontScale=1,
                        color=(0, 0, 255),
                        thickness=1,
                        lineType=cv2.LINE_AA,
                    )

                debug_video.write(image)

    if debug_video_fname is not None:
        debug_video.release()

    return VideoInferenceResult(submission=pd.DataFrame.from_dict(df), raw_predictions=raw_predictions)


class SingleVideoClipsInferenceDataset(Dataset):
    def __init__(self, frames: List[np.ndarray], clip_length: int, clip_step: int, video_name: str):
        self.frames = frames
        self.clip_length = clip_length
        self.clip_step = clip_step
        self.num_samples = int((len(frames) - (clip_length - clip_step)) / self.clip_step)
        self.normalize = A.Normalize(DATASET_MEAN, DATASET_STD)
        self.video_name = video_name

    def __len__(self):
        return self.num_samples

    def __repr__(self):
        f"SingleVideoClipsInferenceDataset(clip_length{self.clip_length}, clip_step={self.clip_step}, num_samples={self.num_samples})"

    def __getitem__(self, index):
        start = index * self.clip_step
        end = start + self.clip_length
        if end > len(self.frames):
            tail = len(self.frames) - end
            start -= tail
            end -= tail
        return self._get_clip(frame_indexes=np.arange(start, end))

    def _get_clip(self, frame_indexes: np.ndarray):
        samples = [self.frames[i] for i in frame_indexes]

        image_stack = []

        video_names = [self.video_name] * len(frame_indexes)
        video_frame_indexes = frame_indexes + 1

        for i, image in enumerate(samples):
            data = {IMAGE_KEY: image}
            data = self.normalize(**data)
            image = data[IMAGE_KEY]
            image_stack.append(image_to_tensor(image))

        result = {
            INDEX_KEY: np.array(frame_indexes),
            VIDEO_NAME_KEY: video_names,
            VIDEO_FRAME_INDEX_KEY: video_frame_indexes,
            #
            IMAGE_KEY: torch.stack(image_stack),
        }

        return result


def extract_frames(video_fname) -> List[np.ndarray]:
    cap = cv2.VideoCapture(video_fname)
    retval, image = cap.read()
    images = []

    while retval:
        images.append(image)
        retval, image = cap.read()

    return images


def add_bottom_right(df):
    df["right"] = df["left"] + df["width"]
    df["bottom"] = df["top"] + df["height"]
    return df


def box_pair_iou(bbox1, bbox2):
    bbox1 = [float(x) for x in bbox1]
    bbox2 = [float(x) for x in bbox2]

    (x0_1, y0_1, x1_1, y1_1) = bbox1
    (x0_2, y0_2, x1_2, y1_2) = bbox2

    # get the overlap rectangle
    overlap_x0 = max(x0_1, x0_2)
    overlap_y0 = max(y0_1, y0_2)
    overlap_x1 = min(x1_1, x1_2)
    overlap_y1 = min(y1_1, y1_2)

    # check if there is an overlap
    if overlap_x1 - overlap_x0 <= 0 or overlap_y1 - overlap_y0 <= 0:
        return 0

    # if yes, calculate the ratio of the overlap to each ROI size and the unified size
    size_1 = (x1_1 - x0_1) * (y1_1 - y0_1)
    size_2 = (x1_2 - x0_2) * (y1_2 - y0_2)
    size_intersection = (overlap_x1 - overlap_x0) * (overlap_y1 - overlap_y0)
    size_union = size_1 + size_2 - size_intersection

    return size_intersection / size_union


def track_boxes(videodf, dist=1, iou_thresh=0.8):
    # most simple algorithm for tracking boxes
    # based on iou and hungarian algorithm
    track = 0
    n = len(videodf)
    inds = list(videodf.index)
    frames = [-1000] + sorted(videodf["frame"].unique().tolist())
    ind2box = dict(zip(inds, videodf[["left", "top", "right", "bottom"]].values.tolist()))
    ind2track = {}

    for f, frame in enumerate(frames[1:]):
        cur_inds = list(videodf[videodf["frame"] == frame].index)
        assigned_cur_inds = []
        if frame - frames[f] <= dist:
            prev_inds = list(videodf[videodf["frame"] == frames[f]].index)
            cost_matrix = np.ones((len(cur_inds), len(prev_inds)))

            for i, ind1 in enumerate(cur_inds):
                for j, ind2 in enumerate(prev_inds):
                    box1 = ind2box[ind1]
                    box2 = ind2box[ind2]
                    a = box_pair_iou(box1, box2)
                    cost_matrix[i, j] = 1 - a if a > iou_thresh else 1
            row_is, col_js = linear_sum_assignment(cost_matrix)
            # assigned_cur_inds = [cur_inds[i] for i in row_is]
            for i, j in zip(row_is, col_js):
                if cost_matrix[i, j] < 1:
                    ind2track[cur_inds[i]] = ind2track[prev_inds[j]]
                    assigned_cur_inds.append(cur_inds[i])

        not_assigned_cur_inds = list(set(cur_inds) - set(assigned_cur_inds))
        for ind in not_assigned_cur_inds:
            ind2track[ind] = track
            track += 1
    tracks = [ind2track[ind] for ind in inds]
    return tracks


def add_tracking(df, dist=1, iou_thresh=0.8):
    # add tracking data for boxes. each box gets track id
    df = add_bottom_right(df)
    df["track"] = -1
    videos = df["video"].unique()

    for video in videos:
        videodf = df[df["video"] == video]
        tracks = track_boxes(videodf, dist=dist, iou_thresh=iou_thresh)
        df.loc[list(videodf.index), "track"] = tracks
    return df


def keep_maximums(df, dist=2, iou_thresh=0.5, column='scores'):
    # track boxes across frames and keep only box with maximum score
    df = add_tracking(df, dist=dist, iou_thresh=iou_thresh)
    df = df.sort_values(['video', 'track', column], ascending=False).drop_duplicates(['video', 'track'])
    return df


def keep_mean_frame(df, iou_thresh: float = 0.35, dist: int = 2):
    df = add_tracking(df, dist=dist, iou_thresh=iou_thresh)
    keepdf = df.groupby(["video", "track"]).mean()["frame"].astype(int).reset_index()
    df = df.merge(keepdf, on=["video", "track", "frame"])
    return df


def temporal_nms(
    df: pd.DataFrame, helmet_tracking_iou_thresh=0.35, helmet_tracking_dist=2, impact_score=0.6, kernel_size: int = 7
) -> pd.DataFrame:
    """

    :param df:
    :param helmet_tracking_iou_thresh:
    :param helmet_tracking_dist:
    :param impact_score:
    :param kernel_size:
    :return:
    """
    # track boxes across frames and keep only box with maximum score
    df = add_tracking(df, dist=helmet_tracking_dist, iou_thresh=helmet_tracking_iou_thresh)
    df = df.sort_values(by="frame").reset_index()

    videos = df["video"].unique()
    for video in videos:
        tracks = df["track"].unique()
        for track in tracks:
            track_df = df[(df["video"] == video) & (df["track"] == track)]

            scores = track_df.scores.values
            scores_max = maximum_filter(scores, kernel_size)

            # scores = torch.from_numpy(track_df.scores.values).unsqueeze(0).unsqueeze(0)
            # scores_max = torch.nn.functional.max_pool1d(scores, kernel_size=kernel_size, stride=1, padding=kernel_size // 2)
            mask = scores == scores_max
            scores_masked = scores * mask

            df.loc[list(track_df.index), "scores"] = scores_masked

    df = df[df.scores > impact_score]
    return df


def clip_fliplr(x):
    return x.flip(4)


def merge_bboxes(boxes, labels, scores, image_shape=(720, 1280), iou_thr=0.7, method="wbf"):
    from ensemble_boxes import weighted_boxes_fusion, soft_nms, non_maximum_weighted

    image_height, image_width = image_shape[:2]

    coords2norm = np.array([[1.0 / image_width, 1.0 / image_height, 1.0 / image_width, 1.0 / image_height]])
    norm2pixels = np.array([[image_width, image_height, image_width, image_height]])

    norm_boxes_list = [b * coords2norm for b in boxes]
    if method == "wbf":
        norm_boxes, scores, labels = weighted_boxes_fusion(norm_boxes_list, scores, labels, iou_thr=iou_thr)
    elif method == "nms":
        norm_boxes, scores, labels = soft_nms(norm_boxes_list, scores, labels, iou_thr=iou_thr)
    elif method == "nmsw":
        norm_boxes, scores, labels = non_maximum_weighted(norm_boxes_list, scores, labels, iou_thr=iou_thr)

    labels = labels.astype(int)
    boxes = norm_boxes * norm2pixels

    return boxes.astype(int), labels, scores


def ensemble_boxes_per_frame(dfs: List[pd.DataFrame], iou_threshold=0.7) -> pd.DataFrame:
    """
    Runs weighted box fusion per each frame to remove duplicates.
    This is 'lazy ensembling' where you concatenate predictions of different models and then union their predictions.
    :param dfs:
    :return:
    """

    dfs = [add_bottom_right(df.copy()) for df in dfs]

    # We assume all data-frames contains same set of videos
    videos = np.unique(dfs[0].video)

    df_result = defaultdict(list)

    for video in videos:

        # Accumulate frames
        video_dfs = []
        frames = set()
        for df in dfs:
            video_df = df[df.video == video]
            frames = frames.union(np.unique(video_df.frame))
            video_dfs.append(video_df)

        gameKey, playID, view = fs.id_from_fname(video).split("_")
        frames = list(sorted(frames))

        for frame in frames:

            bboxes_list, scores_list, labels_list = [], [], []
            for df in video_dfs:
                frame_df = df[df.frame == frame]
                if len(frame_df) == 0:
                    continue

                boxes = np.stack(
                    [frame_df["left"].values, frame_df["top"].values, frame_df["right"].values, frame_df["bottom"].values],
                    axis=1,
                )
                scores = frame_df["scores"].values
                labels = np.zeros(len(boxes))

                bboxes_list.append(boxes)
                scores_list.append(scores)
                labels_list.append(labels)

            if len(bboxes_list) > 1:
                boxes, labels, scores = merge_bboxes(bboxes_list, labels_list, scores_list, iou_thr=iou_threshold, method="wbf")
            else:
                boxes = bboxes_list[0]
                scores = scores_list[0]

            for (x1, y1, x2, y2), score in zip(boxes, scores):
                df_result["left"].append(x1)
                df_result["top"].append(y1)
                df_result["right"].append(x2)
                df_result["bottom"].append(y2)
                df_result["width"].append(x2 - x1)
                df_result["height"].append(y2 - y1)
                df_result["scores"].append(score)
                df_result["frame"].append(frame)
                df_result["video"].append(video)
                df_result["gameKey"].append(gameKey)
                df_result["playID"].append(playID)
                df_result["view"].append(view)

    return pd.DataFrame.from_dict(df_result)


@torch.no_grad()
def run_inference_video_centernet(
    model,
    box_coder: CenterNetBoxCoderWithImpact,
    video_fname: str,
    clip_length: int,
    clip_step: int,
    batch_size=1,
    helmet_threshold_score: float = 0.5,
    impact_threshold_score: float = 0.5,
    image_size_pad_factor=32,
    debug_video_fname=None,
    apply_activation_when_decode=True,
    fp16=False,
) -> VideoInferenceResult:
    """

    :param model:
    :param box_coder:
    :param video_fname:
    :param batch_size:
    :param helmet_threshold_score:
    :param impact_threshold_score:
    :param image_size_pad_factor:
    :param debug_video_fname:
    :param apply_activation_when_decode: Must be False if model wrapped with ApplySigmoidTo
    :return:
    """

    video = os.path.basename(video_fname)
    gameKey, playID, view = fs.id_from_fname(video_fname).split("_")

    frames = extract_frames(video_fname)
    ds = SingleVideoClipsInferenceDataset(frames, clip_length=clip_length, clip_step=clip_step, video_name=video)

    bboxes_per_frame = defaultdict(list)
    helmet_p_per_frame = defaultdict(list)
    impact_p_per_frame = defaultdict(list)

    loader = DataLoader(
        ds,
        batch_size=batch_size,
        num_workers=0,
        pin_memory=True,
        shuffle=False,
        drop_last=False,
        collate_fn=centernet_video_collate,
    )

    for batch in tqdm(loader, desc=os.path.basename(video_fname)):
        image = batch[IMAGE_KEY]
        batch_size, clip_length, channels, rows, cols = image.size()

        image_padded, pad = pad_clips_tensor(image, image_size_pad_factor)

        with torch.cuda.amp.autocast(fp16):
            output = model(image_padded.cuda())

        for i in range(batch_size):
            decoded = box_coder.decode(
                output[CENTERNET_OUTPUT_HEATMAP][i],
                output[CENTERNET_OUTPUT_IMPACT_MAP][i],
                output[CENTERNET_OUTPUT_SIZE][i],
                output[CENTERNET_OUTPUT_OFFSET][i],
                apply_activation=apply_activation_when_decode,
            )
            for j in range(clip_length):
                frame_index = int(batch[VIDEO_FRAME_INDEX_KEY][i][j])

                bboxes = decoded.bboxes
                # Unpad
                bboxes = bboxes - torch.tensor(
                    [pad[0], pad[2], pad[0], pad[2]], device=bboxes.device, dtype=bboxes.dtype
                ).reshape((1, 1, 4))
                # Clamp
                bboxes.clamp_min_(0)
                bboxes[:, :, 0].clamp_max_(cols)  # X
                bboxes[:, :, 1].clamp_max_(rows)  # Y
                bboxes[:, :, 2].clamp_max_(cols)  # X
                bboxes[:, :, 3].clamp_max_(rows)  # Y

                bboxes_per_frame[frame_index].extend(bboxes[j].tolist())
                helmet_p_per_frame[frame_index].extend(decoded.objectness[j].tolist())
                impact_p_per_frame[frame_index].extend(decoded.scores[j].tolist())

    del loader, batch, image, image_padded

    debug_video = None
    if debug_video_fname is not None:
        os.makedirs(os.path.dirname(debug_video_fname), exist_ok=True)
        fourcc = cv2.VideoWriter_fourcc(*"mp4v")
        debug_video = cv2.VideoWriter(debug_video_fname, fourcc, 60, (1280, 720))

    raw_predictions = []
    df = defaultdict(list)

    frames_numbers = sorted(list(bboxes_per_frame.keys()))
    for frame_index in frames_numbers:
        bboxes = np.array(bboxes_per_frame[frame_index]).reshape((-1, 4))
        helmet_p = np.array(helmet_p_per_frame[frame_index]).reshape(-1)
        impact_p = np.array(impact_p_per_frame[frame_index]).reshape(-1)

        if clip_step != clip_length:
            raise NotImplementedError("Here must go NMS")

        raw_predictions.append(
            {
                "frame": frame_index,
                "gameKey": gameKey,
                "playID": playID,
                "video": video,
                "view": view,
                "bboxes": bboxes,
                "helmet_probas": helmet_p,
                "impact_probas": impact_p,
            }
        )

        # Select & filter
        confident_helmet_mask = helmet_p >= helmet_threshold_score
        confident_impact_mask = confident_helmet_mask & (impact_p >= impact_threshold_score)

        helmet_bboxes = to_numpy(bboxes[confident_helmet_mask]).astype(int)
        helmet_probas = to_numpy(helmet_p[confident_helmet_mask])

        impact_bboxes = to_numpy(bboxes[confident_impact_mask]).astype(int)
        impact_probas = to_numpy(impact_p[confident_impact_mask])

        if len(impact_bboxes):
            for (x1, y1, x2, y2), impact_p in zip(impact_bboxes, impact_probas):
                df["gameKey"].append(gameKey)
                df["playID"].append(int(playID))
                df["view"].append(view)
                df["video"].append(video)
                df["frame"].append(frame_index)
                df["left"].append(x1)
                df["width"].append(x2 - x1)
                df["top"].append(y1)
                df["scores"].append(impact_p)
                df["height"].append(y2 - y1)

        if debug_video_fname is not None:

            # Seems that frames starts from 1
            image = frames[frame_index - 1].copy()

            for (x1, y1, x2, y2), helmet_p in zip(helmet_bboxes, helmet_probas):
                cv2.rectangle(image, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)
                cv2.putText(
                    image,
                    text=f"{helmet_p:.2f}",
                    org=(x1, y1 - 5),
                    fontFace=cv2.FONT_HERSHEY_PLAIN,
                    fontScale=1,
                    color=(0, 255, 0),
                    thickness=1,
                    lineType=cv2.LINE_AA,
                )

            for (x1, y1, x2, y2), impact_p in zip(impact_bboxes, impact_probas):
                cv2.rectangle(image, (x1, y1), (x2, y2), color=(0, 0, 255), thickness=2)
                cv2.putText(
                    image,
                    text=f"{impact_p:.2f}",
                    org=(x1, y2 + 10),
                    fontFace=cv2.FONT_HERSHEY_PLAIN,
                    fontScale=1,
                    color=(0, 0, 255),
                    thickness=1,
                    lineType=cv2.LINE_AA,
                )

            debug_video.write(image)

    if debug_video_fname is not None:
        debug_video.release()

    return VideoInferenceResult(submission=pd.DataFrame.from_dict(df), raw_predictions=raw_predictions)


@torch.no_grad()
def run_inference_video_centernet_sliding_window(
    model,
    box_coder: CenterNetBoxCoderWithImpact,
    video_fname: str,
    clip_length: int,
    helmet_threshold_score=0.5,
    impact_threshold_score=0.5,
    image_size_pad_factor=32,
    debug_video_fname=None,
    apply_activation_when_decode=True,
    fp16=False,
) -> VideoInferenceResult:
    """

    :param model:
    :param box_coder:
    :param video_fname:
    :param batch_size:
    :param helmet_threshold_score:
    :param impact_threshold_score:
    :param image_size_pad_factor:
    :param debug_video_fname:
    :param apply_activation_when_decode: Must be False if model wrapped with ApplySigmoidTo
    :return:
    """

    video = os.path.basename(video_fname)
    gameKey, playID, view = fs.id_from_fname(video_fname).split("_")

    frames = extract_frames(video_fname)
    ds = SingleVideoClipsInferenceDataset(frames, clip_length=clip_length, clip_step=1, video_name=video)

    bboxes_per_frame = defaultdict(list)
    helmet_p_per_frame = defaultdict(list)
    impact_p_per_frame = defaultdict(list)

    loader = DataLoader(
        ds,
        batch_size=1,
        num_workers=0,
        pin_memory=True,
        shuffle=False,
        drop_last=False,
        collate_fn=centernet_video_collate,
    )

    running_average = None

    for batch in tqdm(loader, desc=os.path.basename(video_fname)):
        image = batch[IMAGE_KEY]
        batch_size, clip_length, channels, rows, cols = image.size()

        image_padded, pad = pad_clips_tensor(image, image_size_pad_factor)

        with torch.cuda.amp.autocast(fp16):
            output = model(image_padded.cuda())

        if running_average is None:
            running_average = output
        else:
            for fm in [CENTERNET_OUTPUT_HEATMAP, CENTERNET_OUTPUT_IMPACT_MAP, CENTERNET_OUTPUT_SIZE, CENTERNET_OUTPUT_OFFSET]:
                output[fm][:, :-1, ...] += running_average[fm][:, 1:, ...]
                output[fm][:, :-1, ...] *= 0.5

            running_average = output

        decoded = box_coder.decode(
            running_average[CENTERNET_OUTPUT_HEATMAP][0],
            running_average[CENTERNET_OUTPUT_IMPACT_MAP][0],
            running_average[CENTERNET_OUTPUT_SIZE][0],
            running_average[CENTERNET_OUTPUT_OFFSET][0],
            apply_activation=apply_activation_when_decode,
        )

        # Since step size is 1, we accumulate one frame in clip (that has accumulated predictions)
        j = 0  #
        frame_index = int(batch[VIDEO_FRAME_INDEX_KEY][0][j])

        bboxes = decoded.bboxes
        # Unpad
        bboxes = bboxes - torch.tensor([pad[0], pad[2], pad[0], pad[2]], device=bboxes.device, dtype=bboxes.dtype).reshape(
            (1, 1, 4)
        )
        # Clamp
        bboxes.clamp_min_(0)
        bboxes[:, :, 0].clamp_max_(cols)  # X
        bboxes[:, :, 1].clamp_max_(rows)  # Y
        bboxes[:, :, 2].clamp_max_(cols)  # X
        bboxes[:, :, 3].clamp_max_(rows)  # Y

        bboxes_per_frame[frame_index].extend(bboxes[j].tolist())
        helmet_p_per_frame[frame_index].extend(decoded.objectness[j].tolist())
        impact_p_per_frame[frame_index].extend(decoded.scores[j].tolist())

    # And process remaining stuff from the last batch
    decoded = box_coder.decode(
        running_average[CENTERNET_OUTPUT_HEATMAP][0],
        running_average[CENTERNET_OUTPUT_IMPACT_MAP][0],
        running_average[CENTERNET_OUTPUT_SIZE][0],
        running_average[CENTERNET_OUTPUT_OFFSET][0],
        apply_activation=apply_activation_when_decode,
    )
    for j in range(1, clip_length):
        frame_index = int(batch[VIDEO_FRAME_INDEX_KEY][0][j])

        bboxes = decoded.bboxes
        # Unpad
        bboxes = bboxes - torch.tensor([pad[0], pad[2], pad[0], pad[2]], device=bboxes.device, dtype=bboxes.dtype).reshape(
            (1, 1, 4)
        )
        # Clamp
        bboxes.clamp_min_(0)
        bboxes[:, :, 0].clamp_max_(cols)  # X
        bboxes[:, :, 1].clamp_max_(rows)  # Y
        bboxes[:, :, 2].clamp_max_(cols)  # X
        bboxes[:, :, 3].clamp_max_(rows)  # Y

        bboxes_per_frame[frame_index].extend(bboxes[j].tolist())
        helmet_p_per_frame[frame_index].extend(decoded.objectness[j].tolist())
        impact_p_per_frame[frame_index].extend(decoded.scores[j].tolist())

    del loader, batch, image, image_padded

    debug_video = None
    if debug_video_fname is not None:
        os.makedirs(os.path.dirname(debug_video_fname), exist_ok=True)
        fourcc = cv2.VideoWriter_fourcc(*"mp4v")
        debug_video = cv2.VideoWriter(debug_video_fname, fourcc, 60, (1280, 720))

    raw_predictions = []
    df = defaultdict(list)

    frames_numbers = sorted(list(bboxes_per_frame.keys()))
    for frame_index in frames_numbers:
        bboxes = np.array(bboxes_per_frame[frame_index]).reshape((-1, 4))
        helmet_p = np.array(helmet_p_per_frame[frame_index]).reshape(-1)
        impact_p = np.array(impact_p_per_frame[frame_index]).reshape(-1)

        raw_predictions.append(
            {
                "frame": frame_index,
                "gameKey": gameKey,
                "playID": playID,
                "video": video,
                "view": view,
                "bboxes": bboxes,
                "helmet_probas": helmet_p,
                "impact_probas": impact_p,
            }
        )

        # Select & filter
        confident_helmet_mask = helmet_p >= helmet_threshold_score
        confident_impact_mask = confident_helmet_mask & (impact_p >= impact_threshold_score)

        helmet_bboxes = to_numpy(bboxes[confident_helmet_mask]).astype(int)
        helmet_probas = to_numpy(helmet_p[confident_helmet_mask])

        impact_bboxes = to_numpy(bboxes[confident_impact_mask]).astype(int)
        impact_probas = to_numpy(impact_p[confident_impact_mask])

        if len(impact_bboxes):
            for (x1, y1, x2, y2), impact_p in zip(impact_bboxes, impact_probas):
                df["gameKey"].append(gameKey)
                df["playID"].append(int(playID))
                df["view"].append(view)
                df["video"].append(video)
                df["frame"].append(frame_index)
                df["left"].append(x1)
                df["width"].append(x2 - x1)
                df["top"].append(y1)
                df["scores"].append(impact_p)
                df["height"].append(y2 - y1)

        if debug_video_fname is not None:

            # Seems that frames starts from 1
            image = frames[frame_index - 1].copy()

            for (x1, y1, x2, y2), helmet_p in zip(helmet_bboxes, helmet_probas):
                cv2.rectangle(image, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)
                cv2.putText(
                    image,
                    text=f"{helmet_p:.2f}",
                    org=(x1, y1 - 5),
                    fontFace=cv2.FONT_HERSHEY_PLAIN,
                    fontScale=1,
                    color=(0, 255, 0),
                    thickness=1,
                    lineType=cv2.LINE_AA,
                )

            for (x1, y1, x2, y2), impact_p in zip(impact_bboxes, impact_probas):
                cv2.rectangle(image, (x1, y1), (x2, y2), color=(0, 0, 255), thickness=2)
                cv2.putText(
                    image,
                    text=f"{impact_p:.2f}",
                    org=(x1, y2 + 10),
                    fontFace=cv2.FONT_HERSHEY_PLAIN,
                    fontScale=1,
                    color=(0, 0, 255),
                    thickness=1,
                    lineType=cv2.LINE_AA,
                )

            debug_video.write(image)

    if debug_video_fname is not None:
        debug_video.release()

    return VideoInferenceResult(submission=pd.DataFrame.from_dict(df), raw_predictions=raw_predictions)


@torch.no_grad()
def run_inference_video_centernet_8_16(
    model8,
    model16,
    box_coder: CenterNetBoxCoderWithImpact,
    video_fname: str,
    batch_size=1,
    helmet_threshold_score=0.5,
    impact_threshold_score=0.5,
    image_size_pad_factor=32,
    debug_video_fname=None,
    apply_activation_when_decode=True,
    fp16=False,
) -> VideoInferenceResult:
    """
    Special inference routine for combining models with clip size 8 & 16
    :param model:
    :param box_coder:
    :param video_fname:
    :param batch_size:
    :param helmet_threshold_score:
    :param impact_threshold_score:
    :param image_size_pad_factor:
    :param debug_video_fname:
    :param apply_activation_when_decode: Must be False if model wrapped with ApplySigmoidTo
    :return:
    """

    video = os.path.basename(video_fname)
    gameKey, playID, view = fs.id_from_fname(video_fname).split("_")

    frames = extract_frames(video_fname)
    ds = SingleVideoClipsInferenceDataset(frames, clip_length=16, clip_step=16, video_name=video)

    bboxes_per_frame = defaultdict(list)
    helmet_p_per_frame = defaultdict(list)
    impact_p_per_frame = defaultdict(list)

    loader = DataLoader(
        ds,
        batch_size=batch_size,
        num_workers=0,
        pin_memory=True,
        shuffle=False,
        drop_last=False,
        collate_fn=centernet_video_collate,
    )

    for batch in tqdm(loader, desc=os.path.basename(video_fname)):
        image = batch[IMAGE_KEY]
        batch_size, clip_length, channels, rows, cols = image.size()

        image_padded, pad = pad_clips_tensor(image, image_size_pad_factor)

        with torch.cuda.amp.autocast(fp16):
            image_padded = image_padded.cuda()
            output = model16(image_padded)
            output1 = model8(image_padded[:, 0:8, ...])
            output2 = model8(image_padded[:, 8:16, ...])
            for fm in [CENTERNET_OUTPUT_HEATMAP, CENTERNET_OUTPUT_IMPACT_MAP, CENTERNET_OUTPUT_SIZE, CENTERNET_OUTPUT_OFFSET]:
                output[fm][:, 0:8, ...] += output1[fm]
                output[fm][:, 8:16, ...] += output2[fm]
                output[fm] *= 0.5

        for i in range(batch_size):
            decoded = box_coder.decode(
                output[CENTERNET_OUTPUT_HEATMAP][i],
                output[CENTERNET_OUTPUT_IMPACT_MAP][i],
                output[CENTERNET_OUTPUT_SIZE][i],
                output[CENTERNET_OUTPUT_OFFSET][i],
                apply_activation=apply_activation_when_decode,
            )
            for j in range(clip_length):
                frame_index = int(batch[VIDEO_FRAME_INDEX_KEY][i][j])

                bboxes = decoded.bboxes
                # Unpad
                bboxes = bboxes - torch.tensor(
                    [pad[0], pad[2], pad[0], pad[2]], device=bboxes.device, dtype=bboxes.dtype
                ).reshape((1, 1, 4))
                # Clamp
                bboxes.clamp_min_(0)
                bboxes[:, :, 0].clamp_max_(cols)  # X
                bboxes[:, :, 1].clamp_max_(rows)  # Y
                bboxes[:, :, 2].clamp_max_(cols)  # X
                bboxes[:, :, 3].clamp_max_(rows)  # Y

                bboxes_per_frame[frame_index].extend(bboxes[j].tolist())
                helmet_p_per_frame[frame_index].extend(decoded.objectness[j].tolist())
                impact_p_per_frame[frame_index].extend(decoded.scores[j].tolist())

    del loader, batch, image, image_padded

    debug_video = None
    if debug_video_fname is not None:
        os.makedirs(os.path.dirname(debug_video_fname), exist_ok=True)
        fourcc = cv2.VideoWriter_fourcc(*"mp4v")
        debug_video = cv2.VideoWriter(debug_video_fname, fourcc, 60, (1280, 720))

    raw_predictions = []
    df = defaultdict(list)

    frames_numbers = sorted(list(bboxes_per_frame.keys()))
    for frame_index in frames_numbers:
        bboxes = np.array(bboxes_per_frame[frame_index]).reshape((-1, 4))
        helmet_p = np.array(helmet_p_per_frame[frame_index]).reshape(-1)
        impact_p = np.array(impact_p_per_frame[frame_index]).reshape(-1)

        raw_predictions.append(
            {
                "frame": frame_index,
                "gameKey": gameKey,
                "playID": playID,
                "video": video,
                "view": view,
                "bboxes": bboxes,
                "helmet_probas": helmet_p,
                "impact_probas": impact_p,
            }
        )

        # Select & filter
        confident_helmet_mask = helmet_p >= helmet_threshold_score
        confident_impact_mask = confident_helmet_mask & (impact_p >= impact_threshold_score)

        helmet_bboxes = to_numpy(bboxes[confident_helmet_mask]).astype(int)
        helmet_probas = to_numpy(helmet_p[confident_helmet_mask])

        impact_bboxes = to_numpy(bboxes[confident_impact_mask]).astype(int)
        impact_probas = to_numpy(impact_p[confident_impact_mask])

        if len(impact_bboxes):
            for (x1, y1, x2, y2), impact_p in zip(impact_bboxes, impact_probas):
                df["gameKey"].append(gameKey)
                df["playID"].append(int(playID))
                df["view"].append(view)
                df["video"].append(video)
                df["frame"].append(frame_index)
                df["left"].append(x1)
                df["width"].append(x2 - x1)
                df["top"].append(y1)
                df["scores"].append(impact_p)
                df["height"].append(y2 - y1)

        if debug_video_fname is not None:

            # Seems that frames starts from 1
            image = frames[frame_index - 1].copy()

            for (x1, y1, x2, y2), helmet_p in zip(helmet_bboxes, helmet_probas):
                cv2.rectangle(image, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)
                cv2.putText(
                    image,
                    text=f"{helmet_p:.2f}",
                    org=(x1, y1 - 5),
                    fontFace=cv2.FONT_HERSHEY_PLAIN,
                    fontScale=1,
                    color=(0, 255, 0),
                    thickness=1,
                    lineType=cv2.LINE_AA,
                )

            for (x1, y1, x2, y2), impact_p in zip(impact_bboxes, impact_probas):
                cv2.rectangle(image, (x1, y1), (x2, y2), color=(0, 0, 255), thickness=2)
                cv2.putText(
                    image,
                    text=f"{impact_p:.2f}",
                    org=(x1, y2 + 10),
                    fontFace=cv2.FONT_HERSHEY_PLAIN,
                    fontScale=1,
                    color=(0, 0, 255),
                    thickness=1,
                    lineType=cv2.LINE_AA,
                )

            debug_video.write(image)

    if debug_video_fname is not None:
        debug_video.release()

    return VideoInferenceResult(submission=pd.DataFrame.from_dict(df), raw_predictions=raw_predictions)


def kernel_video_inference_main(
    kaggle_data_dir: str,
    checkpoints: List[str],
    activation="after_model",
    tta: Optional[str] = None,
    use_fast_submit=False,
    use_sliding_window=False,
    clip_length=8,
    helmet_threshold_score=0.5,
    impact_threshold_score=0.5,
    tracking_iou_threshold=0.4,
    tracking_frames_distance=2,
    need_maximum_suppression=True,
    batch_size=BATCH_SIZE,
    test=False
):
    if not test:
        checkpoints = checkpoints[:1]
    model, configs, checkpoints, box_coder = ensemble_from_centernet_checkpoints(
        checkpoints,
        sigmoid_outputs=[CENTERNET_OUTPUT_HEATMAP, CENTERNET_OUTPUT_IMPACT_MAP],
        activation=activation,
        tta=tta,
        # extra_model_kwargs=dict(clip_length=clip_length),
    )

    if isinstance(helmet_threshold_score, float):
        helmet_threshold_score = {"Sideline": helmet_threshold_score, "Endzone": helmet_threshold_score}

    if isinstance(impact_threshold_score, float):
        impact_threshold_score = {"Sideline": impact_threshold_score, "Endzone": impact_threshold_score}

    if test:
        video_dir = os.path.join(kaggle_data_dir, "test")
        videos = find_videos_in_dir(video_dir)
    else:
        video_dir = os.path.join(kaggle_data_dir, "train")
        videos = [os.path.join(video_dir, video_fn) for video_fn in pd.read_csv('train_folds_propagate_0.csv').query("fold == 0")['video'].unique().tolist()]
        print(video_dir)

    print(videos)
    raw_predictions = []
    if use_fast_submit and len(videos) == 6:
        submission = pd.read_csv("../input/nfl-models/fast_submission.csv")
    else:
        for video_fname in videos:
            gameKey, playID, view = fs.id_from_fname(video_fname).split("_")

            if use_sliding_window:
                df = run_inference_video_centernet_sliding_window(
                    model.eval(),
                    box_coder,
                    video_fname=video_fname,
                    clip_length=clip_length,
                    helmet_threshold_score=helmet_threshold_score[view],
                    impact_threshold_score=impact_threshold_score[view],
                    apply_activation_when_decode=False,  # Because we already have activation on ensemble,
                    fp16=True,
                )
            else:
                df = run_inference_video_centernet(
                    model.eval(),
                    box_coder,
                    video_fname=video_fname,
                    clip_step=clip_length,
                    clip_length=clip_length,
                    batch_size=batch_size,
                    helmet_threshold_score=helmet_threshold_score[view],
                    impact_threshold_score=impact_threshold_score[view],
                    apply_activation_when_decode=False,
                    fp16=True,
                )
            raw_predictions.extend(df.raw_predictions)

    return raw_predictions


def conv_bn(inp, oup, stride):
    return nn.Sequential(
        nn.Conv3d(inp, oup, kernel_size=3, stride=stride, padding=(1,1,1), bias=False),
        nn.BatchNorm3d(oup),
        nn.ReLU(inplace=True)
    )


class BottleneckResNet(nn.Module):
    expansion = 4

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(BottleneckResNet, self).__init__()
        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm3d(planes)
        self.conv2 = nn.Conv3d(
            planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm3d(planes)
        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm3d(planes * 4)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


class BottleneckShuffleNet(nn.Module):
    def __init__(self, in_planes, out_planes, stride, groups):
        super(BottleneckShuffleNet, self).__init__()
        self.stride = stride
        self.groups = groups
        mid_planes = out_planes//4
        if self.stride == 2:
            out_planes = out_planes - in_planes
        g = 1 if in_planes==24 else groups
        self.conv1    = nn.Conv3d(in_planes, mid_planes, kernel_size=1, groups=g, bias=False)
        self.bn1      = nn.BatchNorm3d(mid_planes)
        self.conv2    = nn.Conv3d(mid_planes, mid_planes, kernel_size=3, stride=stride, padding=1, groups=mid_planes, bias=False)
        self.bn2      = nn.BatchNorm3d(mid_planes)
        self.conv3    = nn.Conv3d(mid_planes, out_planes, kernel_size=1, groups=groups, bias=False)
        self.bn3      = nn.BatchNorm3d(out_planes)
        self.relu     = nn.ReLU(inplace=True)

        if stride == 2:
            self.shortcut = nn.AvgPool3d(kernel_size=(2,3,3), stride=2, padding=(0,1,1))


    def forward(self, x):
        out = self.relu(self.bn1(self.conv1(x)))
        out = channel_shuffle(out, self.groups)
        out = self.bn2(self.conv2(out))
        out = self.bn3(self.conv3(out))

        if self.stride == 2:
            out = self.relu(torch.cat([out, self.shortcut(x)], 1))
        else:
            out = self.relu(out + x)

        return out


def conv3x3x3(in_planes, out_planes, stride=1):
    # 3x3x3 convolution with padding
    return nn.Conv3d(
        in_planes,
        out_planes,
        kernel_size=3,
        stride=stride,
        padding=1,
        bias=False)


def downsample_basic_block(x, planes, stride):
    out = F.avg_pool3d(x, kernel_size=1, stride=stride)
    zero_pads = torch.Tensor(
        out.size(0), planes - out.size(1), out.size(2), out.size(3),
        out.size(4)).zero_()
    if isinstance(out.data, torch.cuda.FloatTensor):
        zero_pads = zero_pads.cuda()

    out = Variable(torch.cat([out.data, zero_pads], dim=1))

    return out


class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(BasicBlock, self).__init__()
        self.conv1 = conv3x3x3(inplanes, planes, stride)
        self.bn1 = nn.BatchNorm3d(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv3x3x3(planes, planes)
        self.bn2 = nn.BatchNorm3d(planes)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


def resnet50(**kwargs):
    """Constructs a ResNet-50 model.
    """
    model = ResNet(BottleneckResNet, [3, 4, 6, 3], **kwargs)
    return model


class ResNet(nn.Module):

    def __init__(self,
                 block,
                 layers,
                 sample_size,
                 sample_duration,
                 shortcut_type='B',
                 num_classes=400):
        self.inplanes = 64
        super(ResNet, self).__init__()
        self.conv1 = nn.Conv3d(
            3,
            64,
            kernel_size=7,
            stride=(1, 2, 2),
            padding=(3, 3, 3),
            bias=False)
        self.bn1 = nn.BatchNorm3d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)
        self.layer1 = self._make_layer(block, 64, layers[0], shortcut_type)
        self.layer2 = self._make_layer(
            block, 128, layers[1], shortcut_type, stride=2)
        self.layer3 = self._make_layer(
            block, 256, layers[2], shortcut_type, stride=2)
        self.layer4 = self._make_layer(
            block, 512, layers[3], shortcut_type, stride=2)
        last_duration = int(math.ceil(sample_duration / 16))
        last_size = int(math.ceil(sample_size / 32))
        self.avgpool = nn.AvgPool3d(
            (last_duration, last_size, last_size), stride=1)
        self.fc = nn.Linear(512 * block.expansion, num_classes)

        for m in self.modules():
            if isinstance(m, nn.Conv3d):
                m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')
            elif isinstance(m, nn.BatchNorm3d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()

    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            if shortcut_type == 'A':
                downsample = partial(
                    downsample_basic_block,
                    planes=planes * block.expansion,
                    stride=stride)
            else:
                downsample = nn.Sequential(
                    nn.Conv3d(
                        self.inplanes,
                        planes * block.expansion,
                        kernel_size=1,
                        stride=stride,
                        bias=False), nn.BatchNorm3d(planes * block.expansion))

        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample))
        self.inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(self.inplanes, planes))

        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        x = self.avgpool(x)

        x = x.view(x.size(0), -1)
        x = self.fc(x)

        return x


def channel_shuffle(x, groups):
    '''Channel shuffle: [N,C,H,W] -> [N,g,C/g,H,W] -> [N,C/g,g,H,w] -> [N,C,H,W]'''
    batchsize, num_channels, depth, height, width = x.data.size()
    channels_per_group = num_channels // groups
    # reshape
    x = x.view(batchsize, groups, 
        channels_per_group, depth, height, width)
    #permute
    x = x.permute(0,2,1,3,4,5).contiguous()
    # flatten
    x = x.view(batchsize, num_channels, depth, height, width)
    return x


class ShuffleNet(nn.Module):
    def __init__(self,
                 groups,
                 width_mult=1,
                 num_classes=400):
        super(ShuffleNet, self).__init__()
        self.num_classes = num_classes
        self.groups = groups
        num_blocks = [4,8,4]

        # index 0 is invalid and should never be called.
        # only used for indexing convenience.
        if groups == 1:
            out_planes = [24, 144, 288, 567]
        elif groups == 2:
            out_planes = [24, 200, 400, 800]
        elif groups == 3:
            out_planes = [24, 240, 480, 960]
        elif groups == 4:
            out_planes = [24, 272, 544, 1088]
        elif groups == 8:
            out_planes = [24, 384, 768, 1536]
        else:
            raise ValueError(
                """{} groups is not supported for
                   1x1 Grouped Convolutions""".format(num_groups))
        out_planes = [int(i * width_mult) for i in out_planes]
        self.in_planes = out_planes[0]
        self.conv1   = conv_bn(3, self.in_planes, stride=(1,2,2))
        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)
        self.layer1  = self._make_layer(out_planes[1], num_blocks[0], self.groups)
        self.layer2  = self._make_layer(out_planes[2], num_blocks[1], self.groups)
        self.layer3  = self._make_layer(out_planes[3], num_blocks[2], self.groups)

        # building classifier
        self.classifier = nn.Sequential(
                            nn.Dropout(0.2),
                            nn.Linear(out_planes[3], self.num_classes)
                            )

    def _make_layer(self, out_planes, num_blocks, groups):
        layers = []
        for i in range(num_blocks):
            stride = 2 if i == 0 else 1
            layers.append(BottleneckShuffleNet(self.in_planes, out_planes, stride=stride, groups=groups))
            self.in_planes = out_planes
        return nn.Sequential(*layers)

    def forward(self, x):
        out = self.conv1(x)
        out = self.maxpool(out)
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = F.avg_pool3d(out, out.data.size()[-3:])
        out = out.view(out.size(0), -1)
        out = self.classifier(out)
        return out


def pad_tensor():
    return Compose([PadIfNeeded((64, 64, 16))], p=1)


def array_min(scalar, b):
    a = scalar * np.ones(b.shape, dtype=int)
    return np.min(np.stack([a, b], axis=1), axis=1)


def array_max(scalar, b):
    a = scalar * np.ones(b.shape, dtype=int)
    return np.max(np.stack([a, b], axis=1), axis=1)


def extract_mini_clip_tensors_v2(images, bboxes, clip_image_size):
    size_x = clip_image_size[0]
    size_y = clip_image_size[1]

    bboxes = bboxes.reshape((-1, 4))

    if len(bboxes.shape) != 2:
        print(bboxes)
    #print(bboxes)
    cx = 0.5 * (bboxes[:, 0:1] + bboxes[:, 2:3])
    cy = 0.5 * (bboxes[:, 1:2] + bboxes[:, 3:4])

    x1 = array_max(0, (cx - size_x// 2).astype(int))
    y1 = array_max(0, (cy - size_y // 2).astype(int))

    x2 = array_min(1280, (x1 + size_x))
    y2 = array_min(720, (y1 + size_y))

    new_bboxes = np.concatenate([x1, y1, x2, y2], axis=1)

    images_tensor = np.stack(images, axis=0)
    mini_clip_tensors = []
    for bbox in new_bboxes:
        x0, y0, x1, y1 = bbox

        mini_clip_tensor = images_tensor[:, y0: y1, x0: x1, :]
        mini_clip_tensors.append(mini_clip_tensor)
        # test
        s0, s1, s2, s3 = mini_clip_tensor.shape
    return mini_clip_tensors


class InferenceHelmetCropDataset(IterableDataset):
    def __init__(
        self,
        frames,
        bboxes,
        overlaps,
        labels,
        frame_numbers,
        unique_ids,

        preloaded_frames,
        video_fname,
        image_size,

        # clip characteristics
        clip_length: int,
        clip_frame_step: int,
        clip_center_frame: int,

    ):
        self.images = frames
        self.image_size = image_size
        self.image_ids = list(map(os.path.basename, frames))
        self.frame_numbers = frame_numbers
        self.bboxes = bboxes
        self.overlaps = overlaps
        self.labels = labels
        self.unique_ids = unique_ids
        self.num_frames = len(frames)
        self.max_frame = max(self.frame_numbers)

        # mini clip characteristics
        self.clip_length = clip_length
        self.clip_frame_step = clip_frame_step
        self.clip_center_frame = clip_center_frame

        self.normalize = A.Normalize(DATASET_MEAN, DATASET_STD)
        self.impacts = np.array(list(np.any(x) for x in self.labels))

        self.current_center_index = 0
        self.clips_stack = []
        self.video_name = video_fname
        self.augmentations = pad_tensor()
        self.preloaded_frames = preloaded_frames

    def __len__(self):
        return sum([len(values) for i, values in enumerate(self.unique_ids)])

    def __repr__(self):
        f"SinglePlayRandomCropFastDataset(clip_length{self.clip_length}, num_samples={self.__len__()}, num_impact_frames={sum(self.impacts)})"

    def load_image(self, index):
        if index < 0 or index >= len(self.preloaded_frames):
            image_shape = (self.image_size[0], self.image_size[1], 3)
            image = np.zeros(image_shape)

        else:
            image = self.preloaded_frames[index]
        return image


    def _get_mini_clips(self, frames, bboxes, labels, unique_ids):
        # gets clips for all helmets in frame
        images = [self.load_image(i - 1) for i in frames]

        image_stack = []
        bboxes_stack = []
        labels_stack = []

        for i, image in enumerate(images):
            input_data = {IMAGE_KEY: image, BBOXES_KEY: bboxes, LABELS_KEY: labels}
            data = input_data
            #data = self.augmentations(**input_data)

            data = self.normalize(**data)

            image = data[IMAGE_KEY]
            bboxes = np.array(data[BBOXES_KEY], dtype=np.float32)
            impact_labels = np.array(data[LABELS_KEY], dtype=np.long)

            image_stack.append(image)
            bboxes_stack.append(bboxes)
            labels_stack.append(impact_labels)

        # and next we extract mini videos
        center_frame = frames[self.clip_center_frame]
        center_frame_bboxes = bboxes_stack[self.clip_center_frame]
        center_frame_labels = labels_stack[self.clip_center_frame]
        center_frame_unique_ids = unique_ids

        mini_clips = extract_mini_clip_tensors_v2(image_stack, center_frame_bboxes, clip_image_size=(64, 64))
        results = []
        for unique_id, mini_clip_tensor, label in zip(center_frame_unique_ids, mini_clips, center_frame_labels):
            if mini_clip_tensor.shape != (16, 64, 64, 3):
                input_data = {'image': np.transpose(mini_clip_tensor, (1, 2, 0, 3))}
                data = self.augmentations(**input_data) # needs 64, 64, 16, 3
                mini_clip_tensor = np.transpose(data['image'], (2, 0, 1, 3))
            result = {
                INDEX_KEY: unique_id,
                VIDEO_NAME_KEY: self.video_name,
                VIDEO_FRAME_INDEX_KEY: center_frame,
                HAS_OVERLAP_KEY: 0,
                #
                IMAGE_KEY: torch.from_numpy(np.transpose(mini_clip_tensor, (3, 0, 1, 2))),
                LABELS_KEY: torch.tensor(label)
            }
            results.append(result)
        return results

    def __iter__(self):
        while self.current_center_index < len(self.unique_ids):

            impact_index = self.current_center_index
            impact_frame = self.frame_numbers[impact_index]
            start = impact_frame - self.clip_center_frame
            end = start + self.clip_length
            frames = np.arange(start, end, self.clip_frame_step)
            bboxes = self.bboxes[self.current_center_index]
            labels = self.labels[self.current_center_index]
            unique_ids = self.unique_ids[self.current_center_index]
            self.clips_stack = self._get_mini_clips(frames, bboxes, labels, unique_ids)
            self.current_center_index += 1
            for item in self.clips_stack:
                yield item


def single_video_to_dataset_args_by_frame(df: pd.DataFrame):
    """
    df: one video dataframe
    """
    bboxes = collections.defaultdict(list)
    targets = collections.defaultdict(list)
    unique_ids = collections.defaultdict(list)
    overlaps = collections.defaultdict(list)
    image_fnames = {}

    videos = np.unique(df.video)

    if len(videos) != 1:
        raise ValueError("Must have only one video" + ",".join(videos))
    df['has_overlap'] = 0
    #df = find_overlaps(df, pad=2)

    for row in df.itertuples(index=True):
        frame = int(row.frame)
        #if frame >= clip_center_frame and frame <= max_frame + 1 - (clip_length - clip_center_frame):
        x1, y1, x2, y2 = row.left, row.top, row.right, row.bottom
        has_overlap = row.has_overlap == 1
        has_impact = row.impact == 1

        bboxes[frame].append((x1, y1, x2, y2))
        overlaps[frame].append(has_overlap)
        targets[frame].append(int(has_impact))
        unique_ids[frame].append(row.Index)

    frame_numbers = list(sorted(unique_ids.keys()))

    bboxes = list(bboxes[x] for x in frame_numbers)
    overlaps = list(overlaps[x] for x in frame_numbers)
    labels = list(targets[x] for x in frame_numbers)
    unique_ids = list(unique_ids[x] for x in frame_numbers)

    return image_fnames, bboxes, overlaps, labels, frame_numbers, unique_ids


@torch.no_grad()
def run_inference_video_classification(
        model,
        videodf,
        video_dir,
        video_fname: str,
        clip_length: int,
        clip_frame_step: int,
        clip_center_frame: int,
        batch_size=1,
        thresh=0.15,
    ):

    preloaded_frames = extract_frames(os.path.join(video_dir, video_fname))

    #videodf = preddf.query("video == @video")
    bbox_params = A.BboxParams(
        format="pascal_voc", min_area=16, min_visibility=0.3, label_fields=[LABELS_KEY], check_each_transform=True,
    )

    ds = InferenceHelmetCropDataset(*single_video_to_dataset_args_by_frame(videodf),
                                    preloaded_frames,
                                    video_fname=video_fname,
                                    clip_length=clip_length,
                                    image_size=(720, 1280),
                                    clip_frame_step=clip_frame_step,
                                    clip_center_frame=clip_center_frame
                                    )
    loader = DataLoader(
        ds,
        batch_size=batch_size,
        num_workers=0,
        pin_memory=True,
        shuffle=False,
        drop_last=False,
    )
    logits = []
    unique_ids = []

    for batch in tqdm(loader, desc=os.path.basename(video_fname)):
        image = batch[IMAGE_KEY]
        output = model(image.cuda())
        cur_unique_ids = batch[INDEX_KEY].numpy()

        cur_logits = output.detach().cpu().numpy()
        unique_ids.extend(cur_unique_ids)
        logits.append(cur_logits)
    logits = np.concatenate(logits, axis=0)
    scores = softmax(logits, axis=1)[:, 1]
    del loader, batch, image
    return unique_ids, scores


@torch.no_grad()
def run_inference_video_classification_tta(
        model,
        videodf,
        video_dir,
        video_fname: str,
        clip_length: int,
        clip_frame_step: int,
        clip_center_frame: int,
        batch_size=1,
        thresh=0.15,
    ):

    preloaded_frames = extract_frames(os.path.join(video_dir, video_fname))

    #videodf = preddf.query("video == @video")
    bbox_params = A.BboxParams(
        format="pascal_voc", min_area=16, min_visibility=0.3, label_fields=[LABELS_KEY], check_each_transform=True,
    )

    ds = InferenceHelmetCropDataset(*single_video_to_dataset_args_by_frame(videodf),
                                    preloaded_frames,
                                    video_fname=video_fname,
                                    clip_length=clip_length,
                                    image_size=(720, 1280),
                                    clip_frame_step=clip_frame_step,
                                    clip_center_frame=clip_center_frame
                                    )
    loader = DataLoader(
        ds,
        batch_size=batch_size,
        num_workers=0,
        pin_memory=True,
        shuffle=False,
        drop_last=False,
    )
    logits = []
    unique_ids = []

    for batch in tqdm(loader, desc=os.path.basename(video_fname)):
        image1 = batch[IMAGE_KEY]
        output1 = model(image1.cuda())
        image2 = torch.flip(image1, dims=(3,))
        output2 = model(image2.cuda())

        cur_unique_ids = batch[INDEX_KEY].numpy()

        cur_logits1 = output1.detach().cpu().numpy()
        cur_logits2 = output2.detach().cpu().numpy()
        cur_logits = 0.5 * (cur_logits1 + cur_logits2)
        unique_ids.extend(cur_unique_ids)
        logits.append(cur_logits)
    logits = np.concatenate(logits, axis=0)
    scores = softmax(logits, axis=1)[:, 1]
    del loader, batch, image1, image2
    return unique_ids, scores


@torch.no_grad()
def run_inference_video_classification_tta_v2(
        model,
        videodf,
        video_dir,
        video_fname: str,
        clip_length: int,
        clip_frame_step: int,
        clip_center_frame: int,
        batch_size=1,
        thresh=0.15,
    ):

    preloaded_frames = extract_frames(os.path.join(video_dir, video_fname))

    #videodf = preddf.query("video == @video")
    bbox_params = A.BboxParams(
        format="pascal_voc", min_area=16, min_visibility=0.3, label_fields=[LABELS_KEY], check_each_transform=True,
    )

    ds = InferenceHelmetCropDataset(*single_video_to_dataset_args_by_frame(videodf),
                                    preloaded_frames,
                                    video_fname=video_fname,
                                    clip_length=clip_length,
                                    image_size=(720, 1280),
                                    clip_frame_step=clip_frame_step,
                                    clip_center_frame=clip_center_frame
                                    )
    loader = DataLoader(
        ds,
        batch_size=batch_size,
        num_workers=0,
        pin_memory=True,
        shuffle=False,
        drop_last=False,
    )
    logits = []
    unique_ids = []
    scores = []

    for batch in tqdm(loader, desc=os.path.basename(video_fname)):
        image1 = batch[IMAGE_KEY]
        output1 = model(image1.cuda())
        image2 = torch.flip(image1, dims=(3,))
        output2 = model(image2.cuda())

        cur_unique_ids = batch[INDEX_KEY].numpy()

        cur_logits1 = output1.detach().cpu().numpy()
        cur_logits2 = output2.detach().cpu().numpy()
        cur_scores = 0.5 * (softmax(cur_logits1, axis=1)[:, 1] + softmax(cur_logits2, axis=1)[:, 1])
        unique_ids.extend(cur_unique_ids)
        scores.append(cur_scores)
    #logits = np.concatenate(logits, axis=0)
    scores = np.concatenate(scores, axis=1)
    del loader, batch, image1, image2
    return unique_ids, scores

def generate_model(checkpoint_path, model_name='shufflenet'):
    if model_name == 'shufflenet':
        model = ShuffleNet(
                groups=3,
                width_mult=1.5,
                num_classes=600)
        model.classifier = nn.Sequential(
                                nn.Dropout(0.9),
                                nn.Linear(model.classifier[1].in_features, 2))
    else:
        model = resnet50(
                num_classes=600,
                shortcut_type='B',
                sample_size=64,
                sample_duration=16)
        #model.fc = nn.Linear(model.fc.in_features, 600)


    model = nn.DataParallel(model.cuda())
    pretrain = torch.load(checkpoint_path, map_location=torch.device('cpu'))
    print(pretrain)
    model.load_state_dict(pretrain, strict=True)
    return model


def get_inference_model(checkpoint_path):
    model = generate_model(checkpoint_path, model_name=CLF_MODEL_NAME)
    model.eval()
    return model


def video_classificatoin_inference_main(
    kaggle_data_dir: str,
    checkpoint: str,
    thresh: float,
    batch_size=1,
    test=True
    ):

    model = get_inference_model(checkpoint)

    boxdf = pd.read_csv('raw_predictions.csv').query("helmet_probas >= @thresh and impact_probas >= @IMPACT_THRESHOLD_SCORE")
    boxdf['impact'] = 0
    videos = boxdf['video'].unique()
    if test:
        video_dir = os.path.join(kaggle_data_dir, "test")
        #videos = os.listdir(video_dir)
    else:
        video_dir = os.path.join(kaggle_data_dir, "train")
        #videos = [os.path.join(video_dir, video_fn) for video_fn in pd.read_csv('train_folds_propagate_0.csv').
        #                                                            query("fold == 0")['video'].unique().tolist()]
        #videos = pd.read_csv('train_folds_propagate_0.csv').query("fold == 0")['video'].unique().tolist()
        print(video_dir)

    all_scores, all_unique_ids = [], []
    for video_fname in videos:
        videodf = boxdf.query("video == @video_fname")
        video_unique_ids, video_scores = run_inference_video_classification(model,
                                                                            videodf,
                                                                            video_dir,
                                                                            video_fname,
                                                                            clip_length=16,
                                                                            clip_center_frame=8,
                                                                            clip_frame_step=1,
                                                                            thresh=thresh,
                                                                            batch_size=batch_size)
        all_scores.extend(video_scores)
        all_unique_ids.extend(video_unique_ids)

    #all_predictions = (np.array(all_scores) >= thresh).astype(int)
    #boxdf.loc[all_unique_ids, 'impact'] = all_predictions
    boxdf.loc[all_unique_ids, 'scores'] = all_scores
    boxdf.to_csv('final_predictions.csv', index=False)
    return boxdf


def kernel_video_inference_main(
    kaggle_data_dir: str,
    checkpoints: List[str],
    activation="after_model",
    tta: Optional[str] = None,
    use_fast_submit=False,
    use_sliding_window=False,
    clip_length=8,
    helmet_threshold_score=0.5,
    impact_threshold_score=0.5,
    tracking_iou_threshold=0.4,
    tracking_frames_distance=2,
    need_maximum_suppression=True,
    batch_size=BATCH_SIZE,
    test=False
):
    if not test:
        checkpoints = checkpoints[:1]
    model, configs, checkpoints, box_coder = ensemble_from_centernet_checkpoints(
        checkpoints,
        sigmoid_outputs=[CENTERNET_OUTPUT_HEATMAP, CENTERNET_OUTPUT_IMPACT_MAP],
        activation=activation,
        tta=tta,
        # extra_model_kwargs=dict(clip_length=clip_length),
    )

    if isinstance(helmet_threshold_score, float):
        helmet_threshold_score = {"Sideline": helmet_threshold_score, "Endzone": helmet_threshold_score}

    if isinstance(impact_threshold_score, float):
        impact_threshold_score = {"Sideline": impact_threshold_score, "Endzone": impact_threshold_score}

    if test:
        video_dir = os.path.join(kaggle_data_dir, "test")
        videos = find_videos_in_dir(video_dir)
    else:
        video_dir = os.path.join(kaggle_data_dir, "train")
        videos = [os.path.join(video_dir, video_fn) for video_fn in pd.read_csv('train_folds_propagate_0.csv').query("fold == 0")['video'].unique().tolist()]
        print(video_dir)

    print(videos)
    raw_predictions = []
    if use_fast_submit and len(videos) == 6:
        submission = pd.read_csv("../input/nfl-models/fast_submission.csv")
    else:
        for video_fname in videos:
            gameKey, playID, view = fs.id_from_fname(video_fname).split("_")

            if use_sliding_window:
                df = run_inference_video_centernet_sliding_window(
                    model.eval(),
                    box_coder,
                    video_fname=video_fname,
                    clip_length=clip_length,
                    helmet_threshold_score=helmet_threshold_score[view],
                    impact_threshold_score=impact_threshold_score[view],
                    apply_activation_when_decode=False,  # Because we already have activation on ensemble,
                    fp16=True,
                )
            else:
                df = run_inference_video_centernet(
                    model.eval(),
                    box_coder,
                    video_fname=video_fname,
                    clip_step=clip_length,
                    clip_length=clip_length,
                    batch_size=batch_size,
                    helmet_threshold_score=helmet_threshold_score[view],
                    impact_threshold_score=impact_threshold_score[view],
                    apply_activation_when_decode=False,
                    fp16=True,
                )
            raw_predictions.extend(df.raw_predictions)

    return raw_predictions


def raw_predictions_to_df(raw_predictions):
    df = pd.DataFrame.from_dict(raw_predictions)
    not_explode_cols = ['frame','gameKey','playID','video','view']
    bboxes = df[not_explode_cols + ['bboxes']]
    helmet_probas = df[not_explode_cols + ['helmet_probas']]
    impact_probas = df[not_explode_cols + ['impact_probas']]
    bboxes_explode = bboxes.explode('bboxes', ignore_index=True)
    helmet_probas_explode = helmet_probas.explode('helmet_probas', ignore_index=True)
    impact_probas_explode = impact_probas.explode('impact_probas', ignore_index=True)
    bboxes_explode['helmet_probas'] = helmet_probas_explode['helmet_probas']
    bboxes_explode['impact_probas'] = impact_probas_explode['impact_probas']
    bb = bboxes_explode
    bb['left'] = bb['bboxes'].map(lambda x: x[0])
    bb['top'] = bb['bboxes'].map(lambda x: x[1])
    bb['right'] = bb['bboxes'].map(lambda x: x[2])
    bb['bottom'] = bb['bboxes'].map(lambda x: x[3])
    bb['width'] = bb['right'] - bb['left']
    bb['height'] = bb['bottom'] - bb['top']
    return bb


def keep_maximums(df, dist=2, iou_thresh=0.5, column='scores'):
    # track boxes across frames and keep only box with maximum score
    df = add_tracking(df, dist=dist, iou_thresh=iou_thresh)
    df = df.sort_values(['video', 'track', column], ascending=False).drop_duplicates(['video', 'track'])
    return df


def submission_postprocess(df, clf_thresh=0.3, helmet_thresh=0.5, impact_thresh=0.05, tracking_iou_thresh=0.1,
                           tracking_dist=5, column='scores'):
    df = df[df['helmet_probas'] >= helmet_thresh]
    df = df[df['impact_probas'] >= impact_thresh]
    df = df[df['scores'] >= clf_thresh]
    df = keep_maximums(df, iou_thresh=tracking_iou_thresh, dist=tracking_dist, column=column)
    df['left'] = df['left'].astype(int)
    df['top'] = df['top'].astype(int)
    df['width'] = df['width'].astype(int)
    df['height'] = df['height'].astype(int)
    return df


def joint_inference(run_detection=True, run_classification=True):
    if run_detection:
        raw_predictions = kernel_video_inference_main(
            kaggle_data_dir=KAGGLE_DATA_DIR,
            checkpoints=CHECKPOINT,
            activation=ACTIVATION_AFTER,
            tta=TTA_MODE,
            use_fast_submit=USE_FAST_SUBMIT,
            use_sliding_window=USE_SLIDING_WINDOW,
            clip_length=CLIP_LENGTH,
            helmet_threshold_score=HELMET_THRESHOLD_SCORE,
            impact_threshold_score=IMPACT_THRESHOLD_SCORE,
            tracking_iou_threshold=TRACKING_IOU_THRESHOLD,
            tracking_frames_distance=TRACKING_FRAMES_DISTANCE,
            batch_size=BATCH_SIZE,
            test=TEST
        )
        pred_df = raw_predictions_to_df(raw_predictions)
        pred_df.to_csv('raw_predictions.csv')
    if run_classification:
        final = video_classificatoin_inference_main(
            kaggle_data_dir=KAGGLE_DATA_DIR,
            checkpoint=CLF_CHECKPOINT,
            thresh=CLF_THRESH,
            batch_size=CLF_BATCH_SIZE,
            test=TEST
        )
        submission = submission_postprocess(final, clf_thresh=CLF_THRESH, helmet_thresh=HELMET_THRESHOLD_SCORE,
                                            impact_thresh=IMPACT_THRESHOLD_SCORE, tracking_iou_thresh=TRACKING_IOU_THRESHOLD,
                                            tracking_dist=TRACKING_FRAMES_DISTANCE, column=COLUMN)
        submission = submission.loc[:, ['gameKey', 'playID', 'view', 'video', 'frame', 'left', 'width', 'top', 'height']]
        submission.to_csv('submission.csv')
    return submission

# Main
joint_inference(run_detection=False, run_classification=True)

